{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Ben,\n",
    "\n",
    "I enjoyed solving the problem and I want to utilise the 24 hours you've given me. Kindly view this notebook as a supplement to the main submission. I start with the cleaned version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "D = pd.read_csv('clean_diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = D.drop(columns=['Unnamed: 0'])\n",
    "D = D.loc[D['Insulin'] < 400] # Remove outliers.\n",
    "D.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine helped functions. It would have been nice if I could have imported them as a module!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trn_tst_split(data, trn_pct):\n",
    "    \"\"\"Split the data set into training and test.\n",
    "    \n",
    "        data: the data frame to be split.\n",
    "        trn_pct: % of training data. Must be between 0 and 1.\n",
    "        \n",
    "        Returns a list of two data frames - training and test.\n",
    "    \"\"\"\n",
    "    random.seed(12111842)\n",
    "    N = data.shape[0]\n",
    "    n_train = int(N * trn_pct)\n",
    "    trn_indices = set(random.sample(range(N), n_train))\n",
    "    tst_indices = set(range(N)) - trn_indices\n",
    "    \n",
    "    assert len(trn_indices.intersection(tst_indices)) == 0\n",
    "    assert len(trn_indices) + len(tst_indices) == N\n",
    "    \n",
    "    # Convert them to lists.\n",
    "    trn_indices = [i for i in trn_indices]\n",
    "    tst_indices = [i for i in tst_indices]\n",
    "    \n",
    "    trn_data = data.iloc[trn_indices, :]\n",
    "    tst_data = data.iloc[tst_indices, :]\n",
    "    \n",
    "    assert trn_data.shape[0] + tst_data.shape[0] == data.shape[0]\n",
    "    \n",
    "    return [trn_data, tst_data]\n",
    "\n",
    "\n",
    "def print_cm_results(cm):\n",
    "    \"\"\"Print diagnostics from the confusion matrix.\"\"\"\n",
    "    recall = cm[0, 0]/(cm[0, 0] + cm[1, 0])\n",
    "    precision = cm[0, 0]/(cm[0, 0] + cm[0, 1])\n",
    "    specificity = cm[1, 1]/(cm[1, 0] + cm[1, 1])\n",
    "    f1_score = 2*recall*precision/(recall + precision)\n",
    "    accuracy = np.trace(cm)/np.sum(cm)\n",
    "    \n",
    "    print(f'% +ves correctly predicted (recall) = {round(recall * 100, 2)}')\n",
    "    print(f'% +ves detected out of all (precision) = {round(precision * 100, 2)}')\n",
    "    print(f'% -ves detected out of all (specificity) = {round(specificity * 100, 2)}')\n",
    "    print(f'f1 score = {round(f1_score, 2)}')\n",
    "    print(f'accuracy = {round(accuracy * 100, 2)}')\n",
    "    \n",
    "def get_model_matrices(D, formula):\n",
    "    return dmatrices(formula, data=D, return_type='dataframe')\n",
    "\n",
    "def prob_to_outcome(y, threshold = 0.5):\n",
    "    \"\"\"Converts probability to a class depending on the threshold.\"\"\"\n",
    "    if y < threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def build_cm(yt, yp):\n",
    "    \"\"\"Generates confusion matrix from actual and predicted values.\"\"\"\n",
    "    yt['predicted'] = [prob_to_outcome(y) for y in yp]\n",
    "    yt.columns = ['actual', 'predicted']\n",
    "    yt['actual'] = yt['actual'].apply(lambda f: int(f))\n",
    "    cm = pd.crosstab(yt['actual'], yt['predicted']).to_numpy()\n",
    "    \n",
    "    return cm    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps there is a library in Python to carry out cross-validation. But I don't yet know about it. Therefore, I implemented a few functions to carry out 30-fold cross validation. We have 750 observations. I build a model using 725 of them and use 25 to test. I choose the 25 observations to test sequentially. In the first iteration rows 0-24 are test rows, in the second one 25-49 are test rows, and so on. In each iteration, I compute the accuracy of the model and the model coefficients. I save them in two lists.\n",
    "\n",
    "Finally, I will take an average of the model coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = np.linspace(0, D.shape[0], 26)\n",
    "all_params = []\n",
    "all_accuracies = []\n",
    "\n",
    "for i in range(len(boundaries) - 1):\n",
    "    l = int(boundaries[i])\n",
    "    r = int(boundaries[i+1])\n",
    "    print(f'{l}, {r}')\n",
    "    D_test = D.iloc[l:r, :]\n",
    "    D_train = pd.concat([D.iloc[0:l, :], D.iloc[r:D.shape[0], :]], axis=0)\n",
    "    yt, Xt = get_model_matrices(D_test, f_v2)\n",
    "    yn, Xn = get_model_matrices(D_train, f_v2)\n",
    "    model = sm.Logit(yn, Xn)\n",
    "    results = model.fit()\n",
    "    yp = results.predict(Xt)\n",
    "    cm = build_cm(yt, yp)\n",
    "    \n",
    "    all_accuracies.append(np.trace(cm)/np.sum(cm))\n",
    "    all_params.append(results.params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([round(a, 2) for a in all_accuracies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params_df = pd.concat(all_params, axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem, t\n",
    "\n",
    "def get_mean_with_ci(X):\n",
    "    m = np.mean(X)\n",
    "    conf = 0.95\n",
    "    N = len(X)\n",
    "    h = sem(X) * t.ppf((1 + conf)/2, N - 1)\n",
    "    \n",
    "    return (m, m - h, m + h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final results of the exercise are the outputs of the two cells below. In particular, when we put the model in production, we will use the mean values of the coefficients printed in the last cell. The 95% confidence interval of the model's accuracy is printed in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(m, l, r) = get_mean_with_ci(all_accuracies)\n",
    "print(f'Accuracies mean = {round(m, 2)}, C.I. = [{round(l, 2)}, {round(r, 2)}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_intervals = []\n",
    "all_rows = []\n",
    "\n",
    "for i in range(all_params_df.shape[0]):\n",
    "    row = all_params_df.iloc[i, :].to_numpy()\n",
    "    name = row[0]\n",
    "    all_rows.append(row)\n",
    "    info = get_mean_with_ci(row[1:len(row)])\n",
    "    print(f'{name}: coeff-mean = {round(info[0], 3)}, C.I. = [{round(info[1], 3)}, {round(info[2], 3)}]')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
