---
title: "F distribution"
author: "Amey Joshi"
date: "16/11/2019"
format: html
editor: visual
---

## Comparison of variances

Consider a standard normally distributed population of size $1000$. Suppose we draw two samples from it, one of size $n_1$ and another of size $n_2$. We compute the sample variances and their ratio $r = s_1^2/s_2^2$. The quantity $r$ is $F$- distributed with parameters $\nu_1 = n_1 - 1$ and $\nu_2 = n_2 - 1$.

```{r}
library(MASS)
set.seed(18081932)

N <- 1000
population <- rnorm(N)

n1 <- 15
n2 <- 20

n.samples <- 500

sample_vars <- function(i) {
  s1 <- population[sample(N, n1, replace = TRUE)]
  s2 <- population[sample(N, n2, replace = TRUE)]
  var(s1)/var(s2)
}

sample.f <- sapply(1:n.samples, sample_vars)
approx.mean <- round(summary(sample.f)[4], 1)
X1 <- seq(from = 0, to = approx.mean, length.out = 100)
X2 <- seq(from = approx.mean, to = 10, length.out = n.samples - length(X1))
X <- c(X1, X2)
Y <- df(X, n1 - 1, n2 - 1)
plot(X, Y, type = "l", col = 1, lty = 1)
lines(density(sample.f), col = 2, lty = 2)
legend.text <- c("Theoretical", "Actual")
legend("topright", legend = legend.text, lty = c(1, 2), col = c(1, 2), cex = 0.6)
title.txt <- paste(sep = "", "F(", n1 - 1, ", ", n2 - 1, ") distribution")
title(title.txt)
```

An alternative interpreation of the $F$-statistic is that it is the ratio of two appropriately scaled $\chi^2$ random variables. Recall that $\chi^2$ random variable is the sum of squares of normal random variables so that its ratio with its degrees of freedom is the variance of the random variables. Thus, if $U_1$ and $U_2$ are two $\chi^2$ distributed random variables with $d_1$ and $d_2$ degrees of freedom then $U_1/d_1$ and $U_2/d_2$ are the respective variances.

## A test of equality of variances

Let us draw two samples from our population of size $n_1$ and $n_2$ and compute the ratio of their variances. We know that we have drawn them from the same population and therefore we should not by surprised if their variances are close enough. Yet, for the moment, let us pretend that we did not know how the samples came about.

```{r}
s1 <- rnorm(n1)
s2 <- rnorm(n2)

v1 <- var(s1)
v2 <- var(s2)
F_s <- v1/v2
cat(paste(sep = "", "The ratio of variances is ", F_s, ".\n"))
```

If this ratio is less than $1$, the probability of getting an even lesser ratio will be the value of the distribution function at this ratio. If it is greater than $1$, the probability of getting a more extreme value is the area under the $F$-density curve beyond the ratio. If we call it $p_1$.

```{r}
p_1 <- ifelse(F_s < 1, pf(F_s, n1 - 1, n2 - 1), 1 - pf(F_s, n1 - 1, n2 - 1))
```

If we had swapped the names of the sample, the probability of getting a more extreme value (on either sides) would have been,

```{r}
p_2 <- ifelse(F_s < 1, 1 - pf(1/F_s, n2 - 1, n1 - 1), pf(1/F_s, n2 - 1, n1 - 1))
```

As these two possibilities are mutually exclusive, the probability of getting ratios more extreme than those produced by these samples is

```{r}
p <- p_1 + p_2
```

In other words,

```{r}
cat(paste(sep = "","The p-value of our test is ", round(p, 5), ".\n"))
```

R provides a test for comparing variances of samples.

```{r}
var.test(s1, s2)
```

Our $p$-value matches with that of the test. Since it is so large, we fail to reject the null hypothesis.

Let us repeat the exercise with samples whose variances are expected to be unequal.

```{r}
s3 <- rnorm(n1)
s4 <- rnorm(n2, sd = 2)

v3 <- var(s3)
v4 <- var(s4)
F_s <- v3/v4
cat(paste(sep = "", "The ratio of variances is ", F_s, ".\n"))
p_1 <- ifelse(F_s < 1, pf(F_s, n1 - 1, n2 - 1), 1 - pf(F_s, n1 - 1, n2 - 1))
p_2 <- ifelse(F_s < 1, 1 - pf(1/F_s, n2 - 1, n1 - 1), pf(1/F_s, n2 - 1, n1 - 1))
p <- p_1 + p_2
cat(paste(sep = "","The p-value of our test is ", round(p, 5), ".\n"))
```

On the other hand, the variance test of R gives

```{r}
var.test(s3, s4)
```

Once again the $p$-values match but this time we reject the null hypothesis.
