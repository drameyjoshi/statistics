---
title: "Multi-layer perceptron"
author: "Amey Joshi"
date: "2/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(nnet)
```

## Why multiple layers?

- We demonstrated the perceptron using a data set where classes were indeed
  separated by a linear boundary. But we are not always so lucky.
- Way back in 1969, it was known that the perceptron could not classify the 
  XOR problem.
- People discovered that this problem can be overcome by stacking perceptrons
  one over the other so that the inputs of one of them are the outputs of the
  ones below. Hence the name *Multi-layer Perceptrons*. The layers between
  the input and output are called the *hidden layers*.
- A network with two or more hidden layers is called a *deep neural network*. 
- The first MLPs were trained using back-propagation algorithm. However, there
  are many others now.
- We will demonstrate the use of multi-layer perceptron by using it on the 
  iris data.


## Preparing the data

- How many cases do we have for each class (species)?
```{r}
aggregate(cbind(n.cases = Sepal.Length) ~ Species, iris, length)
```

- There are an equal number of cases in each class. Let us prepare our sample
  so that we choose an equal number from each class.
```{r}
set.seed(18121842)
iris.setosa <- iris[iris$Species == 'setosa',]
iris.versicolor <- iris[iris$Species == 'versicolor',]
iris.virginica <- iris[iris$Species == 'virginica',]

split_data <- function(N, p) {
  stopifnot(p > 0 & p < 1)
  n <- N * p
  trn.index <- sample.int(N, n, replace = FALSE)
  test.index <- setdiff(1:N, trn.index)
  list(train = trn.index, test = test.index)
}

setosa.split <- split_data(nrow(iris.setosa), 0.2)
setosa.train <- iris.setosa[setosa.split[["train"]],]
setosa.test  <- iris.setosa[setosa.split[["test"]],]

versicolor.split <- split_data(nrow(iris.versicolor), 0.2)
versicolor.train <- iris.versicolor[versicolor.split[["train"]],]
versicolor.test  <- iris.versicolor[versicolor.split[["test"]],]

virginica.split <- split_data(nrow(iris.virginica), 0.2)
virginica.train <- iris.virginica[virginica.split[["train"]],]
virginica.test  <- iris.virginica[virginica.split[["test"]],]

train.data <- rbind(setosa.train, versicolor.train, virginica.train)
test.data  <- rbind(setosa.test, versicolor.test, virginica.test)

rm(
  setosa.split,
  versicolor.split,
  virginica.split,
  iris.setosa,
  iris.versicolor,
  iris.virginica
)
rm(setosa.train, versicolor.train, virginica.train)
rm(setosa.test, versicolor.test, virginica.test)
```

## Training the network - 1

- We use the single hidden layer network provided in B.D. Ripley's 'nnet' package.
- Recall that in the perceptron we had a hyperparameter $\eta$ called the learning rate.
  It influenced the rate at which weights changed in each iteration. We use the relation
  $$w_{i+1} = w_i - \eta\frac{\partial E}{\partial w_i}$$ to updates weights in gradient
  descent.
- Sometimes the weights can get extreme. To avoid that, we apply a regularizing parameter
  $\lambda$ called the decay. With it, the gradient descent is modified to
  $$w_{i+1} = w_i - \eta\frac{\partial E}{\partial w_i} - \eta\lambda w_i.$$ 
 - As in the case of the perceptron, we can also specify the maximum number of iterations.
 - The hidden layer is specified by passing its size.
 
## Training the network - 2

- The function call to build the single hidden layer neural network is
```{r}
nn.1 <-
  nnet(
    Species ~ .,
    data = train.data,
    size = 2,
    decay = 1e-5,
    maxit = 50
  )
```

- We have not dropped any feature from the data for we do not know which of them
  are useful in a 3-way classification.
- One can see the network using the summary function.
```{r}
summary(nn.1)
```

- We observe that there are four input neurons i1, i2, i3, i4, one for each feature.
  There is also a 'bias' neuron b. 
- There are two neurons in the hidden layer h1 and h2, along with a bias b
- As expected, there are three output layers o1, o2, o3.
- The weights from one neuron to the others in the succeeding layer are also mentioned.
- The output layer is treated to a softmax function to convert it to a probability.

## Testing the network

- Like other models in R we have the predict function for neural networks as well.
```{r}
predicted.species <- predict(nn.1, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)

# How did we do?
table(comparison)
```

- We mistook a all virginica to be versicolor.
- Does increasing the number of iteration help?

## Model with increasing the number of iterations
```{r}
nn.2 <-
  nnet(
    Species ~ .,
    data = train.data,
    size = 2,
    decay = 1e-5,
    maxit = 100
  )

predicted.species <- predict(nn.2, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)

# How did we do?
table(comparison)
```

- It did, but not too much. How about increasing the hidden layers.

## Model with more units in the hidden layer.
```{r}
nn.3 <-
  nnet(
    Species ~ .,
    data = train.data,
    size = 4,
    decay = 1e-5,
    maxit = 50
  )
predicted.species <- predict(nn.3, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)

# How did we do?
table(comparison)
```

- Adding hidden layers made no great difference.

## Model with more units and more iterations.
```{r}
nn.4 <-
  nnet(
    Species ~ .,
    data = train.data,
    size = 4,
    decay = 1e-5,
    maxit = 50
  )
predicted.species <- predict(nn.4, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)

# How did we do?
table(comparison)
```

- There is no change.

## Model with fewer features.
```{r}
nn.5 <-
  nnet(
    Species ~ Petal.Length + Petal.Width,
    data = train.data,
    size = 4,
    decay = 1e-5,
    maxit = 50
  )

predicted.species <- predict(nn.5, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)
table(comparison)

```

- Dropping features made a great difference. Can we improve?

## Model with fewer features and more iterations.
```{r}
nn.6 <-
  nnet(
    Species ~ Petal.Length + Petal.Width,
    data = train.data,
    size = 4,
    decay = 1e-5,
    maxit = 100
  )

predicted.species <- predict(nn.6, test.data, type = "class")
comparison <- data.frame(actual = test.data$Species, predicted = predicted.species)
table(comparison)
```

- No change.

## Closing remarks
- An MLP with a single layer has far more hyperparameters to play with.
- Sometimes, dropping features gives better results. In this case, visualizing the
  data helped us select the features.