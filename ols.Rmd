---
title: "Ordinary Least Square Regression"
author: "Amey Joshi"
date: "22/11/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\ev}{\operatorname{E}}
\newcommand{\var}{\operatorname{var}}
\newcommand{\cov}{\operatorname{cov}}

## The data set
We will illustrate linear regression using the 'cars' data set. It has the 
distance taken to stop a car moving at a certain speed. In this experiment, 
speed is a deterministic variable and the distance it travelled before stopping 
is a random variable. Before trying to fit a regression model we will first 
check if a model can at all be built. The easiest first step is to visualize the 
data.
```{r}
with(cars, plot(dist, speed, main = "Stopping distance of cars"))
```

The scatter plot shows that there is a positive correlation between the two
variables. The correlation coefficient is
```{r}
with(cars, cor(speed, dist))
```

There is a high enough correlation between the two variables. Yet one sees a 
significant scatter along with a positive trend. It might be fruitful to check
if there are any outliers in the data.
```{r}
par(mfrow = c(1, 2))
with(cars, boxplot(speed, main = "Speed"))
with(cars, boxplot(dist, main = "Distance"))
```

The distance data seems to have an outlier. We can find it using the box-plot 
statistics.
```{r}
boxplot.stats(cars$dist)$out
```

The outlier rows are
```{r}
cars[which(cars$dist %in% boxplot.stats(cars$dist)$out), ]
```

For the moment we wil not worry about the sole outlier in our data set.

Let us also examine the distribution of the two variables.
```{r}
library(MASS)
truehist(cars$speed, nbins = 10, xlab = "Speed")
lines(density(cars$speed))
rug(cars$speed)
```

The distribution is symmetric but we do not know if it is gaussian. We check it
using the Shapiro-Wilk test whose null hypothesis is that the data is normal.
```{r}
shapiro.test(cars$speed)
```

The high p-value suggests that the data is indeed normal. 

```{r}
truehist(cars$dist, nbins = 10, xlab = "Distance")
lines(density(cars$dist))
rug(cars$dist)
```

The outlier detected in the boxplot is visible in the histogram as well. The
distribution appears to be skewed to the right. Perhaps the distribution is 
not normal. If we run the Shapiro-Wilk test, 
```{r}
shapiro.test(cars$dist)
```

the low $p$-value allows us to reject the null hypothesis that the data is normal.

## The theory of regression
In its simplest case, when a single deterministic variable $x$ is used to predict
$y$ using a linear relationship between the two, regression consists in fitting
the 'best' line passing through the given data points $(x_i, y_i)$. If $\hat{y}_i$
is the value of $y$ predicted by a linear relationship, we want to minimize the
'error',
\begin{equation}\label{e1}
E = \sum_{i=1}^N (y_i - \hat{y}_i)^2,
\end{equation}
where $N$ is the number of data points. If $y$ is modelled as a linear function
of $x$ then we want to fit a line
\begin{equation}\label{e2}
\hat{y}_i = \beta_1 x_i + \beta_0
\end{equation}
such that $E$ is minized. The values of $\beta_1$ and $\beta_0$ are found using
the relations
\begin{eqnarray}
\frac{\partial E}{\partial\beta_0} &=& 0 \label{e3} \\
\frac{\partial E}{\partial\beta_1} &=& 0 \label{e4}
\end{eqnarray}
They are
\begin{equation}\label{e5}
\beta_0 = \frac{\sum_{i=1}^N x_i^2\sum_{i=1}^Ny_i - \sum_{i=1}^N x_i\sum_{i=1}^N x_iy_i}{N\sum_{i=1}^N x_i^2 - \left(\sum_{i=1}^N x_i\right)^2}
\end{equation}
and
\begin{equation}\label{e6}
\beta_1 = \frac{N\sum_{i=1}^N x_iy_i- \sum_{i=1}^N x_i\sum_{i=1}^N y_i}{N\sum_{i=1}^N x_i^2 - \left(\sum_{i=1}^N x_i\right)^2}
\end{equation}

## The regression model
We will now build and diagnose a linear regression model on the cars data.
```{r}

m.1 <- lm(dist ~ speed, cars)
summary(m.1)
```

We confirm that the values of the slope and the intercept are indeed those
predicted by the two equations derived in the previous section.
```{r}
sum.x <- sum(cars$speed)
sum.y <- sum(cars$dist)
sum.x.sq <- sum(cars$speed^2)
sum.xy <- cars$speed %*% cars$dist
N <- nrow(cars)

den <- N * sum.x.sq - (sum.x)^2
num.1 <- N * sum.xy - sum.x * sum.y
num.0 <- sum.x.sq * sum.y - sum.x * sum.xy

beta.1 <- num.1/den
beta.0 <- num.0/den
cat(paste("Slope", round(beta.1, 4), "\n"))
cat(paste("Intercept", round(beta.0, 4), "\n"))
```

We show the data and the fitted linear model in the previous plot. 
```{r}
with(cars, plot(speed, dist))
abline(m.1)
```

We next look at the residuals.
```{r}
plot(m.1$residuals, ylab = "residual", main = "Model residuals")
```

The mean and standard error of residuals is
```{r}
residual.mean <- mean(m.1$residuals)

# Subtracting 2 because we fitted 2 parameters, slope and the intercept
residual.df <- length(m.1$residuals) - 2 
residual.se <- sqrt(sum(m.1$residuals^2)/residual.df)
```

The residuals seems to be distributed evenly on either side of zero. There does
not appear to be a trend in them. To find out if they are normally distributed 
we examine their Q-Q plot.
```{r}
qqnorm(m.1$residuals)
qqline(m.1$residuals)
```

The data may not be far from normal. Let us also see the histogram and the 
density of the residuals before subjecting them to Shapiro-Wilk test.
```{r}
library(MASS)
truehist(m.1$residuals, xlab = "residual", main = "Histogram and density of residuals", nbins = 10)
lines(density(m.1$residuals))
```

The residuals are skewed to the right. 
```{r}
shapiro.test(m.1$residuals)
```

The small $p$-value suggests that we reject the null hypothesis that the residuals
are normally distributed must be rejected. This shatters one of the assumptions
of a linear model. Nevertheless, we look at other aspects.

## Further diagnostics
We will now understand the other numbers printed while summarizing the linear
model. Before we get there, we recall the formulae for $\beta_0$ and $\beta_1$.
They depends on the $N$ data points $(x_i, y_i)$. A different sample of the $N$
data points will give (hopefully slightly) different values of the intercept
$\beta_0$ and the slope $\beta_1$. Thus, the estimates of slope and intercept
are themselves 'statistics' and therefore they too have their sampling 
distributions. The ones computed using equations \eqref{e5} and \eqref{e6} are
thus the estimates of the true intercept and slope. In order to differentiate
an estimate from the true value, we fill follow the standard convention of 
putting a hat on the estimates.

It is easy to get an expression for the mean and variance of these
estimates if we recast the expressions for $\beta_0$ and $\beta_1$ in a slightly
different form. We start with $\beta_1$. Its numerator $n_1$ can be written as
\begin{equation}\label{e7}
n_1 = N^2\ev(xy) - N^2\ev(x)\ev(y).
\end{equation} 
Now, 
\begin{equation}\label{e8}
\ev((x - \ev(x))(y - \ev(y)) = 
\ev(xy) - \ev(x)\ev(y) - \ev(x)\ev(y) + \ev(x)\ev(y) = 
\ev(xy) - \ev(x)\ev(y).
\end{equation}
By the same principles, the denominator $d$ of the expression for $\beta_1$ can
be written as 
\begin{equation}\label{e9}
d = \ev (x - \ev x)^2.
\end{equation}
Thus,
\begin{equation}\label{e10}
\hat{\beta_1} = \frac{N^2\ev((x - \ev x)(y - \ev y))}{N^2\ev((x - \ev x)^2)}  
= \frac{\sum_{i=1}^N(x_i - \ev x)(y_i - \ev y)}{\sum_{i=1}^N{(x_i - \ev x)^2}}
\end{equation}

The denominators of $\beta_0$ and $\beta_1$ are identical. Let us denote the common
denominator by $d$ and let us write $\beta_0 = n_0/d$ and $\beta_1 = n_1/d$. Now
\begin{equation}\label{e11}
n_1\sum_{i=1}^Nx_i = N\sum_{i=1}^Nx_i\sum_{i=1}^Nx_iy_i - \left(\sum_{i=1}^Nx_i\right)^2\sum_{i=1}^Ny_i
\end{equation}
so that 
\begin{equation}\label{e12}
\sum_{i=1}^Nx_i\sum_{i=1}^Nx_iy_i = n_1\ev{x} + \left(\sum_{i=1}^Nx_i\right)^2\ev{y}.
\end{equation}
We can
now write 
\begin{equation}\label{e13}
n_0 = \sum_{i=1}^N x_i^2\sum_{i=1}^N y_i - n_1\ev x + \left(\sum_{i=1}^Nx_i\right)^2\ev y
= N\sum_{i=1}^N x_i^2\ev y - n_1\ev x + \left(\sum_{i=1}^Nx_i\right)^2\ev y = d\ev y - n_1\ev x.
\end{equation}
The estimate of intercept is thus
\begin{equation}\label{e14}
\hat{\beta}_0 = \frac{n_0}{d} = \ev y - \beta_1\ev x.
\end{equation}

We will now state the expression for the variance of the estimated regression 
coefficients.
\begin{eqnarray}
\var(\hat{\beta}_0) &=& \sigma^2\left(\frac{1}{N} + \frac{\ev x}{\sum_{i=1}^N(x_i - \ev x)^2}\right) \label{e15} \\
\var(\hat{\beta}_1) &=& \frac{\sigma^2}{\sum_{i=1}^N(x_i - \ev x)^2} \label{e16}
\end{eqnarray}
$\sigma^2$ in these equations is the variance of the residuals computed using
$N - 2$ degrees of freedom. The reason for choosing $N - 2$ and not $N - 1$ is
explained a little later.

We will now confirm the formulae of equations \eqref{e15} and \eqref{e16}.
```{r}
x.bar <- mean(cars$speed)
ss.x <- sum((cars$speed - x.bar)^2)
N <- nrow(cars)

sigma.sq <- var(m.1$residuals) * (N-1)/(N-2)

se.beta.0 <- sqrt(sigma.sq * (1/N + x.bar^2/ss.x))
se.beta.1 <- sqrt(sigma.sq/ss.x)

cat(paste("Standard error of intercept:", round(se.beta.0, 4), "\n"))
cat(paste("Standard error of slope:", round(se.beta.1, 4), "\n"))
```

Now that we have the standard errors for the estimates, we can get their 
$t$-values as well. The $t$-values are computed to test the null hypothesis
that the coefficients are zero. As a result, we assume that the true mean is 
zero and we estimate the probability of finding a value that we estimated.
```{r}
t.beta.0 <- (beta.0 - 0)/se.beta.0
t.beta.1 <- (beta.1 - 0)/se.beta.1

cat(paste("t-value of intercept:", round(t.beta.0, 4), "\n"))
cat(paste("t-value of slope:", round(t.beta.1, 4), "\n"))
```

How good the model is can be judged by comparing its estimates with those of 
a 'null-model', that is the one whose coefficients are zero. The predictions
of the 'null-model' are the random noise introduced by the standard normally
distributed $\epsilon_i$. We will now introduce the concept of variance due to
the model and that due to random noise. Let
\begin{equation}\label{e17}
SS = \sum_{i=1}^N (y_i - \bar{y})^2,
\end{equation}
be the sum of squares of the predicted variables. The model predicts $\hat{y}_i$
for $y_i$. Therefore, the 'explained sum of squares' is
\begin{equation}\label{e18}
ESS = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2.
\end{equation}
The difference between $SS$ and $ESS$ is called the residual sum of squares, 
$RSS$. Let us get them for our model.
```{r}
SS <- sum((cars$dist - mean(cars$dist))^2)
ESS <- sum((fitted(m.1) - mean(cars$dist))^2)
RSS <- SS - ESS

cat(paste("SS =", SS, "\n"))
cat(paste("ESS =", ESS, "\n"))
cat(paste("RSS =", RSS, "\n"))
```

The residuals also available as a part of model summary. We can readily compute
$RSS$ from them.
```{r}
RSS.model <- sum(m.1$residuals^2)
```

The variance $ESS$ arises from choosing the two parameters that we did. The
number of degrees of freedom giving rise to it is $2$. Therefore, the mean
variance due to the model is $ESS/1 = ESS$. Let us call it 'explained variance'.
Thus,
```{r}
explained.variance <- ESS/(2 - 1)
```

The total number of degrees of freedom is $N - 1$, of which $2 - 1$ are taken
up by $ESS$. Therefore, the number of degrees of freedom taken by $RSS$ is $N - 2$.
The 'unexplained variances' is thus,
```{r}
unexplained.variance <- RSS/(N - 2)
```

The ratio os variances gives us the $F$-statistic for the model. The number of
degrees of freedom are $1$ for the numerator and $48$ for the denominator.
```{r}
F.stat <- explained.variance/unexplained.variance
cat(paste("F statistic =", F.stat, "\n"))
```

The $p$-value for the model is
```{r}
p.val <- 1 - pf(F.stat, 1, 48)
cat(paste("p-value for the model =", p.val, "\n"))
```

Both these numbers match the one given by the model's summary.


