\documentclass{article}
\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{hyperref}
\title{Elementary Probability}
\date{09-Mar-2023}
\author{Amey Joshi}
\begin{document}
\maketitle
A mathematical treatment of probability requires a clear definition of a
sample space, events and an assignment of probability to the events. We
will start our discussion with discrete sample spaces. We will focus on
quickly understanding the key concepts and applying them to practical 
problems in statistics.

We require the theory of probability when we cannot predict the outcome of
an experiment. Sometimes it is theoretically possible to predict the 
outcome but doing so is not practically feasible. For instance, we 
understand classical mechanics sufficiently well to accurately predict 
whether a coin will land heads up if we have enough details about the coin
and the way it was tossed. We can, after all, predict and control the 
motion of rockets in outer space. In most practical situations, it is
easier to conduct a large number of experiments and estimate the outcome of
future experiments.

Since we decided to start with discrete sample spaces we will naturally 
restrict ourselves to experiments which have a countable number of 
outcomes. For example
\begin{enumerate}
\item A toss of a coin in which we allow the coin to land heads or tails 
up. We do not consider the possibility of the coin at rest in the vertical
plane. We can describe the sample space by $\{H, T\}$.
\item A die when rolled on a flat table will have one of its six faces up.
The sample space is $\{1, 2, 3, 4, 5, 6\}$.
\item Number of typos in a book. Although in all practical situations this
number is finite it is helpful if consider the number to be indefinitely
large. We can allow the sample space to be $\mathbb{N}$, the set of all
natural numbers.
\item The number of mutations in a gene will have the same sample space
as above.
\item Consider a man whose senses have largely abandoned him. If he were
left at a lamppost and permitted to walk along a straigh line passing 
through the lamppost then in his inebriated state, he may move either
toward or away from the lamppost at every step. If we let the origin be
at the lamppost and align the $x$-axis along the path he takes, then at
every step he either moves along the positive of the negative $x$-axis.
After $n$ steps, he can be anywhere between $-n$ and $+n$. Once again, it
is convenient to consider the sample space to be the set of all integers,
$\mathbb{Z}$.

Although stated in casual terms this is a genuinely interesting problem in
the theory of probability. It is called the \emph{random walk} problem. It
is used to model several phenomena like change in asset price or Brownian
motion of pollen grains in a fluid. 
\end{enumerate}

A sample space is just the set of all possible outcomes of an experiment.
The subsets of the sample space are called events. Elementary events are
singleton subsets of the (discrete) sample space. We assign probabilities
to the elementary events and using them we compute probabilities of other
events. We will illustrate this process for the sample spaces considered
above.
\begin{enumerate}
\item The sample space is $\Omega = \{H, T\}$. We assign probabilities
\begin{eqnarray}
P(H) &=& p \label{e1} \\
P(T) &=& q \label{e2}
\end{eqnarray}
such that 
\begin{equation}\label{e3}
p, q \ge 0
\end{equation}
and 
\begin{equation}\label{e4}
p + q = 1.
\end{equation}
If $p = q = 1/2$, we declare the coin to be fair. However, we can always 
start with an arbitrary choice of $p$ and $q$ subject to equations 
\eqref{e3} and \eqref{e4}, conduct an experiment and revise our estimates.

\item The sample space is $\Omega = \{1, 2, 3, 4, 5, 6\}$ and we can 
assign probability $p_i$ to each $i \in \Omega$ such that
\begin{equation}\label{e5}
p_i \ge 0 \;\forall\; i \in \Omega
\end{equation}
and
\begin{equation}\label{e6}
\sum_{i=1}^6 p_i = 1.
\end{equation}
Once again we can assign arbitrary values to the probabilites subject to
the constraints of equations \eqref{e5} and \eqref{e6}. We then conduct
systematic experiments to determine if our assignment was correct. The
die is declared to be fair only if each $p_i = 1/6$.

\item The sample space is $\mathbb{N}$ and we can assign probabilities
$p_n$ for all $n \in \mathbb{N}$ such that 
\begin{equation}\label{e7}
p_n \ge 0 \;\forall\; n \in \Omega
\end{equation}
and
\begin{equation}\label{e8}
\sum_{n=1}^\infty p_n = 1.
\end{equation}
If the number of words in the book is $N$ then even if we allow the book
to have $m$ typos per word, we can let $p_n$ to be $0$ for all $n > mN$.
Note that our definition of $\mathbb{N}$ includes the number zero. An
initial assignment of probabilities can once again be confirmed by doing
an experiment.

\item The sample space and assignment of probabilities for the number of
mutations in a gene is similar.

\item The sample space in a one dimensional random walk problem is the
set of all integers. We can assign probabilities $p_i \ge 0$ for all $i
\in \mathbb{Z}$ such that 
\begin{equation}\label{e9}
\sum_{i=-\infty}^{\infty} p_i = 1.
\end{equation}
As $0$ is a permissible probability, we can choose sample spaces much 
larger than the practically achievable values of the distance from the 
origin.
\end{enumerate}

The assignment of probabilities to the elementary events of a sample space
is called the probability mass function. In mathematical terms, it is a
function $P: \Omega \rightarrow [0, 1]$ such that
\begin{equation}\label{e10}
P(\omega) \ge 0, \;\forall\; \omega \in \Omega
\end{equation}
and
\begin{equation}\label{e11}
\sum_{\omega \in \Omega}P(\omega) = 1.
\end{equation}

Often times while dealing with \emph{finite} sample spaces we consider all
outcomes to be equally probable. This is called the assumption of equal
\emph{a priori} probability. If the sample space if of size $N$ then the
probability of an event is the ratio of the number of ways in which the 
event can happen and $N$. The calculation of probabilities them reduces to
a problem of counting, that is combinatorics. Such problems are usually
easy to state but quite challenging to solve.

We will now state the axioms of probability theory in the language of set
theory. We considered our sample space $\Omega$ to be discrete. An event
is a subset $E \subseteq \Omega$. The set of all events is usually denoted
by $\mathcal{F}$ and the probability function by $P$. The triple $(\Omega,
\mathcal{F}, P)$ is called a probability space. The probability function
$P$ satisfies the following axioms first proposed by Andrey Kolmogorov
in 1933.
\begin{enumerate}
\item[(A1)] $P(E) \ge 0$ for all $E \in \mathcal{F}$.
\item[(A2)] $P(\Omega) = 1$.
\item[(A3)] If $E_1, E_2, \ldots$ are disjoint event then
\begin{equation}\label{e12}
P\left(\bigcup_{i \ge 1} E_i\right) = \sum_{i \ge 1}P(E_i).
\end{equation}
\end{enumerate}

A few remarks are in order:
\begin{enumerate}
\item When $\Omega$ is a finite set then $\mathcal{F}$ is the set of all
subsets, also called the \emph{power set} of $\Omega$.
\item When $\Omega$ is countable (discrete but infinite) or uncountable 
then the power set is exceedingly large. Many members of the power set are
not interesting events. In such situation, we choose far fewer subsets 
that satisfy certain properties. We will consider them when we study the
theory of probability more rigorously.
\item The union in \eqref{e12} is called a countable union. This seems to
be a trivial choice for simple problems but it is not so when $\Omega$ is
not finite.
\item While writing equation \eqref{e10} we defined $P: \Omega \rightarrow
[0, 1]$. Over there, we were assigning probabilities only to the elementary
events in a discrete sample space. In general, we would want to assign
probabilities to non-elementary events as well. Therefore, we let the domain
of $P$ be all events of interest, that is $\mathcal{F}$. Its range is 
always $[0, 1]$.
\end{enumerate}

A mathematically rigorous probability theory gets into deep waters very
quickly. We will mention the spots where things can get tricky but will
avoid lingering there for too long.

Some sample spaces are ordered, others are not. By an ordered sample space
we mean a set on which the $<$ relation makes sense. If $\Omega = \{H, T\}$
then there is no natural ordering between its members. Likewise, if 
$\Omega$ is the set of all colours of a brand of cars then there is no
natural ordering of its members. On the other hand, the sample space of
typos in a book is $\mathbb{N}$ and for any pair $m, n \in \mathbb{N}$, 
exactly one of the three possibilities $m < n, m = n, m > n$ is true. Such
a set is called an ordered set. For an ordered sample space, one can 
define the \emph{cumulative distribution function} as
\begin{equation}\label{e13}
F(x) = \sum_{\omega \le x}P(\omega).
\end{equation}
The cumulative distribution function may seem to be an unnecessary 
appendage to the theory if we confine ourselves to discrete sample spaces.
It shines when we consider continuous sample spaces, to which we turn our
attention now.

Consider the second hand of a clock. If you look at the clock at a random
instant of time the probability that the second hand is between 2 and 3
is $1/12$. There are $12$ intervals on the dial and when you look at the 
clock at an arbitrary moment it could be in any one of them. Now suppose 
that the dial is divided in to $N$ sections. Then the probability of 
spotting the second hand in any given section is $1/N$. As the number $N$
increases, the sections become shorter and when $N \rightarrow \infty$ a
section reduces to a point. The probability that the second hand is at a
given geometrical point is therefore exactly zero. If this is so how come
the probability of finding the second hand somewhere on the dial is $1$?
Herein lies the subtle difference between uncountable sets and countable
ones. A rigorous analysis of these matters demands a deeper understanding
of \emph{real analysis}. We will, therefore, stay an informal level and 
the first point we will note is that when the sample space is continuous,
it does not make sense to ask the probability at a point. The sensible 
question, in this case, is to ask the probability of something (that thing
is called a random variable) taking a value in an interval. In the case of
the example at hand, the correct question is, what is the probability that
I can find the second hand between angles $\theta_1$ and $\theta_2$?

To understand this point better, consider an analogy. If you have a gram 
of sand then you can ask what is the mass of each grain. You cannot ask 
the same question if you have a gram of water. Instead, you ask what is the
mass of a given droplet of water. If $\rho$ is the density of water then
the mass of the droplet is
\begin{equation}\label{e14}
m = \int_{x_1}^{x_2}\int_{y_1}^{y_2}\int_{z_1}^{z_2}\rho(x, y, z)dxdydz,
\end{equation}
where the droplet extends from $x_1$ to $x_2$ along the $x$-axis, $y_1$
to $y_2$ along the $y$-axis and $z_1$ to $z_2$ along the $z$-axis. You 
don't have to know how to solve such an integral. Just make sure that you
understand what it means.

Likewise, when you have a discrete sample space you can ask the probability
of a random variable taking a particular value. If you know the probability
at all values you know the \emph{probability mass function}. On the other
hand, when you have a continuous sample space you can only ask what is
the probability that a random variable takes values in an interval $[a, b]$
and the answer to that question is
\begin{equation}\label{e15}
p = \int_a^b f(x)dx.
\end{equation}
Continuing the analogy, the function $f$ is called the \emph{probability
density function}.

It is often convenient to assume that the domain of the probability density 
function is $-\infty$ to $\infty$. The probability that a random variable
$X$ takes any value up to a certain number $a$ is
\begin{equation}\label{e16}
P(X \le a) = F(a) = \int_{-\infty}^a f(x)dx.
\end{equation}
The function $F$ is called the cumulative distribution function. From the
fundamental theorem of calculus equation \eqref{e16} also implies that
\begin{equation}\label{e17}
f(x) = \frac{dF}{dx}.
\end{equation}

The usage of continuous sample spaces is not particularly difficult than
the discrete ones. In some ways it is also easier because often times
problems in calculus are easier than those in combinatorics. We will remain
at as mere users and not delve deeper into the mathematical nuances. At
that level the continous sample spaces demand a much greater understanding
of the theory of sets and their sizes, also called their 'measure'.

We close our elementary treatment of probability with an introduction to
conditional probability and Bayes theorem. The ideas are common place and
of common experience. Supposing you go to a doctor complaining of dizziness,
nausea, restlessness and sweating. An experienced physician will not 
immediately conclude that you have a heart attack. Instead, he might ask 
you to raise you arms. He may also ask you if you can walk without support.
If you cannot then he might conclude that you are more likely to have 
vertigo than heart attack. He might nevertheless do additional tests to
ensure that there is nothing life threatening but he might not consider
yours to be an emergency case. 

The probability of an event, here a heart attack, changes as one gets more
information about the patients condition. If the probability was $2/3$ when
the doctor read the first symtoms, it might be $1/3$ by the time he suspects
vertigo. One can state these observations as
\begin{itemize}
\item The probability that the patient has a heart attack given that he is
feeling dizzy, nauseated, restless and sweating is $2/3$.
\item The probability that the patient has a heart attach given that he has
all the above symptoms plus he is unable to walk steadily but he can move
his left arm freely is $1/3$.
\end{itemize}
The phrase 'probability given that' is called 'conditional probability'. We
will make it more formal in terms of sample spaces and events. Let $\Omega$
be a sample space and $A$ and $B$ be events with probability $P(A)$ and
$P(B)$. These are called unconditional probabilities. Now suppose you know
that the event $B$ has occurred and you want to know the probability of 
event $A$. That event $B$ has occurred immediately shrinks your sample 
space to $B$ and the probability of $A$ is proportional to the overlap
between $B$ and $A$. The overlap between $B$ and $A$ is the event that
$B$ and $A$ both occur. In set theoretical language, it is the probability
of the event $A \cap B$. Remember that we said that the probability of $A$
occurring knowing that $B$ has occurred is \emph{proportional to} 
$P(A \cap B)$. The actual probability is
\begin{equation}\label{e18}
P(A | B) = \frac{P(A \cap B)}{P(B)}.
\end{equation}
Note that
\begin{equation}\label{e19}
P(B | A) = \frac{P(B \cap A)}{P(A)} = \frac{P(A \cap B)}{P(A)} \ne
P(A | B).
\end{equation}
We often confuse $P(A|B)$ with $P(B|A)$. We will illustrate it with a
few examples.
\begin{enumerate}
\item Suppose that a certain test is done to check if a patient has a 
disease D. The outcome of the test could be '+' or '-'. The patient could
have the disease or not have it. We denote the first event as $D$ and the
second one as $D^c$, the complement of $D$. No test is perfect. Sometimes 
a patient with the disease may get a '-' result and a healthy person may
get a '+' result. The probabilities of these happening are mentioned in 
the table below.\footnote{This example comes from Larry Wasserman's \emph{
All of Statistics, volume 1.}}
\begin{center}
\begin{tabular}{c | c c}
 & $D$ & $D^c$ \\
\hline
'+' & 0.009 & 0.099 \\
'-' & 0.001 & 0.891
\end{tabular}
\end{center}
Let us understand this table before we analyse it. The unconditional 
probability that a patient has disease is the sum of the first column $
0.009 + 0.001 = 0.01$. That is, $1 \%$ of people in a population have the
disease. Likewise, the unconditional probability that a person is disease
free is the sum of the second column $0.099 + 0.891 = 0.990$. $99\%$ of 
people in a population are disease free. The two probabilities add up to 
$1$.

If we select a person at random and carry out the test the probability of
getting a positive outcome is $0.009 + 0.099 = 0.108$ or $10.8\%$. The 
probability of getting a negative outcome is $0.001 + 0.891 = 0.892$ or
$89.2\%$. One again the sum of the probabilities of two mutually exclusive
outcomes is $1$.

How to interpret the four numbers independently?
\begin{enumerate}
\item The probability that a person has disease and test is '+' is $0.009$.
\item The probability that a person has disease and test is '-' is $0.001$.
\item The probability that a person is healthy and test is '-' is $0.099$.
\item The probability that a person is healthy and test is '+' is $0.891$.
\end{enumerate}

How good is the test?
\begin{enumerate}
\item If a person has disease then the probability that the test result is
'+' is
\begin{equation}\label{e20}
P(+|D) = \frac{P(+ \cap D)}{P(D)} = \frac{0.009}{0.009 + 0.001} = 0.9.
\end{equation}
If a person has the disease there is a $90\%$ chance that the test will be 
positive.
\item If the person is healthy then the probability that the test result is
'-' is
\begin{equation}\label{e21}
P(-|D^c) = \frac{P(+ \cap D^c)}{P(D^c)} = \frac{0.891}{0.099 + 0.891} = 0.9.
\end{equation}
If a person is healthy then there is a $90\%$ chance that the test will be
negative.
\end{enumerate}
These two cases suggest that the test is quite reliable. In practise, 
however, these numbers are not useful. We are not testing the test but the
patient. We want to know the probability that the person has disease given
that the test is '+'. It is
\begin{equation}\label{e23}
P(D|+) = \frac{P(D \cap +)}{P(+)} = \frac{0.009}{0.009 + 0.099} = 0.0833.
\end{equation}
The answer is surprising but correct. If the test is positive then there is
only an $8\%$ chance that you have the disease. That is why medical test
reports always remind their readers to correlate test results with clinical
symptoms. If you do not walk into a medical centre, get yourself tested
and see a '+' outcome then do not immediately conclude that you have the
disease.

\item We now analyse the data about the number of arrests in the UK. The 
summarisation and filtering of raw data from one source, selection 
complementary data from another source and computing the conditional 
probabilities is demonstrated in \href{https://docs.google.com/spreadsheets/d/1tPOcdYhDACN7974XZW4ho0eRbLCEAa3xUChZX5bPOPo/edit?usp=sharing}{a Google
sheet}.
\end{enumerate}
\end{document}
