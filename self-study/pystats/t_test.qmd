# The $t$-test

We will consider one of the simplest statistical test and examine important
topics related to hypothesis testing. We will illustrate the concepts using the
'sleep' dataset.

## The data set {#sec-c2s1}
The 'sleep' dataset has observations of the extra sleep induced by two drugs on
ten subjects. The attribute `extra` records the additional sleep. It may be
negative. The distributuion of `extra` among the subjects when they were
administered a drug looks like
```{python fig.width=8, fig.height=6}
import numpy as np
import rdatasets as rds
import matplotlib.pyplot as plt
from scipy import stats

sleep = rds.data("sleep")
axes = sleep[["group", "extra"]].hist("extra", by="group", sharey=True)

g = 1
for ax in axes.flatten():
    ax.set_xlabel("extra")
    ax.set_ylabel("frequency")
    ax.set_title(f"Group {g}")
    g += 1

_ = plt.suptitle("Extra sleep induced by two drugs")
```
The action of the two drugs on the subjects is clearly not identical. In the 
first case, the range of `extra` is from $-2$ to $4$ while in the second case it
is from $-1$ to $6$. The second drug is likely to be more effective. However, we
do not know if this variation is significant or just out of randomness. Before
further analysis, we will look at the summary of the two groups.

```{python}
sleep[sleep["group"] == 1]["extra"].describe()
```
```{python}
sleep[sleep["group"] == 2]["extra"].describe()
```
The values of mean and median of `extra` in the two groups seem to be quite
different. We are emboldened to suspect that the two drugs indeed have a
differing effectiveness.

## The pooled variance {#sec-c2s2}
To test if the two means are same, we have to compute the pooled variance of
the groups. It is a method of estimating the variance of the population by
assuming the two groups have same variance. In our case, the two variances are
```{python}
vs = [sleep[sleep["group"] == i]["extra"].var().item() for i in [1, 2]]
vr = vs[0]/vs[1]
_ = [print(f"Variance of group {i+1} = {vs[i]:0.4}") for i in [0, 1]]
_ = print(f"Ratio of variances is {vr:0.3}")

```

It is not possible to tell if these sample variances are a manifestation of
sampling from the same population by merely reading their values and their
ratio. However, we can carry out a statistical test that tells us the probability
of getting a ratio of this magnitude if the two variances are equal. That brings
us to the first example of hypothesis testing. Since it involves deciding
something about the ratio of two variances, we first need to find a probability
distribution that describes its variation.

### The F-distribution
Recall the discussion in section @sec-c1s2 that the variance $s^2$ of a sample
of size $k$ drawn from a population with variance $\sigma^2$ has
$$
\frac{s^2}{\sigma^2} \sim \frac{\chi^2_{k-1}}{m},
$$
where $m = k - 1$ if the population was normally distributed. Otherwise, it is
a function of $k$ and $\kappa$, the kurtosis of the population. Now suppose that
there are two such samples drawn of size $k_1$ and $k_2$. Then
$$
\frac{s_i^2}{\sigma^2} \sim \frac{\chi^2_{k_i - 1}}{m(k_i, \kappa)}, i = 1, 2.
$$
Note that the two samples are drawn from the same population. As a result, the
kurtosis used to get $m$ is the same. The ratio of the variances is
$$
\frac{s_1^2}{s_2^2} \sim
\frac{\chi^2_{k_1 - 1}}{\chi^2_{k_2 - 1}}\frac{m(k_2, \kappa)}{m(k_1, \kappa)}
$$
(The algebra of random variables is drastically different from that of numbers.
It is a rather involved topic a glimpse of which can be had by reading the
article [Ratio distribution](https://en.wikipedia.org/wiki/Ratio_distribution).
We will see a example of its subtlety in the cases considered below.)

We consider a few special cases of this distribution.

- If the samples are drawn from a normal population then
$$
\frac{s_1^2}{s_2^2} \sim
\frac{\chi^2_{k_1 - 1}}{\chi^2_{k_2 - 1}}\frac{k_2 - 1}{k_1 - 1}.
$$

- If the sample sizes are equal,
$$
\frac{s_1^2}{s_2^2} \sim \frac{\chi^2_{k_1 - 1}}{\chi^2_{k_1 - 1}}.
$$
Note that the rhs is not equal to the constant $1$. The two samples are not
identical and they will, in general, have unequal variances.

- If the samples are large enough then $m$ will approximately be the same for
them and we can approximate the distribution of the ratio of variances by
the expression in the previous point. How large is enough can be determined by
consulting this plot
```{python}
ratios = [(k - 3)/(k - 1) for k in range(3, 51)]
plt.plot([k for k in range(3, 51)], ratios)
plt.xlabel("k")
plt.ylabel(r"$(k-3)/(k-1)$")
plt.hlines(y=1, xmin=3, xmax=50, linestyle="dashed", color="black")
```
For our immediate purpose, we need the second of the cases discussed above.
Since our main goal right now is to understand hypothesis testing, we will not
halt to discuss the most general form of the distribution of ratio of sample
variances.

If $X_1$ and $X_2$ are $\chi^2$-random variables with degrees of freedom $d_1$
and $d_2$ then the variable $X_1/X_2$ is said to be F-distributed with
parameters $d_1$ and $d_2$. Thus, the ratio of sample variances is F-distributed.

Recall that a $\chi^2$ random variable of $k$ degrees of freedom is the sum of
squares of $k$ independent standard normal random variables. It represents the
variance of a sample $k + 1$ standard normal random variables. The $F$
distribution thus assumes that numerator and denominator are variances of two
samples drawn from the same population.

### Are the two variances equal?
Coming back to the `sleep` data, we calculated the ratio of variances of the
two groups to be $0.7983$. This ratio is our test statistic $t$. Our null
hypothesis is that the two samples are drawn from the same population and
therefore have the same variance. Under the null hypothesis, we can find the
probability that we can get a value of the test statistic as extreme as we got.

```{python}
dfs = sleep[["group", "extra"]].groupby("group").count().to_numpy().flatten()
# Multiplication by 2 is necessary for a 2-way test. We are checking for the
# ratio greater than 1 and less than 1.
t = stats.f.pdf(vr, dfn=dfs[0], dfd=dfs[1])
print(f"The probability of getting a ratio {vr:0.4} of variances is {t:0.4}")
```
In the case of our data, we conclude that if the two samples came from the same
population then a ratio as extreme as we got is possible with a probability of
$0.7286$, which is quite high. In other words, if we reject the null hypothesis
then there is a $72.86\%$ chance that we could be wrong. Therefore, we choose
not to reject the null hypothesis.

Unfortunately, Python does not have an equivalent of `var.test` in R.
This discussion also brings out the limitations of statistical tests like
`var.test`, or the F-test as we used it here, that depend on the properties of 
the underlying probability distribution.

- The data we are studying may not fit the underlying probability model.
- The distribution may not be approximated by a gaussian distribution.
- The parameters of the distribution may not be known.

Statistical tests that depends on the properties of underlying distribution form
what is called as \emph{parametric statistics}. In the recent years, there has
been a shift towards \emph{nonparametric statistics}. The machine learning
algorithms fall in this category.

And yet, there is great value in parametric methods. One must only be careful in
examining whether the assumptions of the tests are valis before running them and
interpreting their results.

We applied hypothesis testing to the problem of testing whether two sample
variances are equal. We will repeat the exercise to test whether the two drugs
in `sleep` data induce the same amount of extra sleep in the ten subjects on
which it was tested.

### The formula for the pooled variance
We have now convinced ourselves that there is a good chance that the two
samples are drawn from the same population. Yet, the magnitudes of their variances
are different. Can we use them to estimate the variance of the population? We
need it to calculate the $t$-statistic.

A pooled variance is an estimator for the population variance when we use the
variance from several samples each one with a different sample mean. If $s_1^2,
\ldots, s_n^2$ are variances of samples of size $k_1, \ldots, k_n$ then the
pooled variance is
$$
s_p^2 = \frac{(k_1 - 1)s_1^2 + \cdots + (k_n - 1)s_n^2}{k_1 - 1 + \cdots k_n - 1}
$$
In our case, since the two samples have the same size, the pooled variance is
just the arithmetic mean of the two sample variances.

When we had just one sample of size $n$, we constructed the $t$ statistic as
$$
t = \frac{\bar{X}_n - \mu}{s}\sqrt{n}.
$$
Now that we have multiple samples (right now, two) we need a way to generalise
this. The correct generalisation is
$$
t = \frac{\bar{X}_n - \mu}{s_p}\sqrt{n_p},
$$
where
$$
\frac{1}{n_p} = \frac{1}{k_1} + \cdots + \frac{1}{k_n}.
$$
Thus, the pooled sample size is the harmonic mean of the individual sample sizes
of the pool.

### Combined variance
There is a related statistic called \emph{combined variance}. It is the variance
of multiple samples around the mean of all sample means. In our case, we have
\begin{eqnarray}
\bar{X}_n &=& \frac{X_1 + \cdots + X_n}{n} \\
\bar{X}_n &=& \frac{X_{n+1} + \cdots + X_{n+m}}{m} \\
\bar{X} &=& \frac{X_1 + \cdots + X_{n+m}}{n + m}
\end{eqnarray}
so that
$$
\bar{X} = \frac{n\bar{X}_n + m\bar{X}_m}{n + m}.
$$
The combined variance is
$$
s_c^2 = \frac{1}{n+m-1}\sum_{i=1}^{n+m}(X_i - \bar{X})^2.
$$
We can manipulate the sum so that
$$
s_c^2 = \frac{n-1}{n+m-1}s_n^2 + \frac{m-1}{n+m-1}s_m^2 +
\frac{mn}{(m+n)(m+n-1)}(\bar{X}_n - \bar{X}_m)^2.
$$

## $t$-test {#sec-c2s3}
We will now show how a $t$ test works. We want to investigate whether the
extra sleep induced by the two drugs is same. We construct the $t$-statistic
as
$$
t = \frac{\bar{X} - \bar{Y}}{s_p}\sqrt(n_p),
$$
where $\bar{X}$ is the sample mean of `extra` in the first group, $\bar{Y}$
is the sample mean of `extra` in the second group, $s_p$ is the pooled variance
and $n_p$ is the pooled degrees of freedom. We calculate each of these as
```{python}
# Group means
grpmns = sleep[["group", "extra"]].groupby("group").mean().to_numpy().flatten()
X_bar = grpmns[0]
Y_bar = grpmns[1]

grpvars = sleep[["group", "extra"]].groupby("group").var().to_numpy().flatten()
v1 = grpvars[0]
v2 = grpvars[1]

grpsize = sleep[["group", "extra"]].groupby("group").count().to_numpy().flatten()
df1 = grpsize[0] - 1
df2 = grpsize[1] - 1

# Pooled variance
v_p = (df1 * v1 + df2 * v2)/(df1 + df2)
n_p = np.prod(grpsize)/np.sum(grpsize)

t_stat = (X_bar - Y_bar)/np.sqrt(v_p) * np.sqrt(n_p)
# We are testing if the means are different. Therefore, we need a 2-way test.
p_val = stats.t.cdf(t_stat, df = df1 + df2) * 2
print(f"t-stat = {t_stat:0.4}, p = {p_val:0.6}, df = {df1 + df2}")
```

A $p$-value of close to $8\%$ indicates that under the null distribution
there is an $8\%$ chance that the $t$-value will be as extreme as this one. If
our significance level is $5\%$, we may want to not reject the null hypothesis.

We can, of course, do all of this using a single function.
```{python}
res = stats.ttest_ind(sleep[sleep["group"] == 1]["extra"],
                      sleep[sleep["group"] == 2]["extra"])
print(f"t-stat = {res.statistic:0.4}, p = {res.pvalue:0.6}, df={res.df:0}")
```

## The paired $t$-test {#sec-c2s4}
We will now use the same data and ask a different question. Supposing the two
groups consisted of the same individuals and the test involved giving them the
two drugs in succession and recording the extra sleep they got, can we tell if
one drug was better than the other?

To answer this question, we set the null hypothesis that there was no difference
in the effect of the two drugs. Therefore, if we take a difference of the extra
sleep induced by the two drugs for each person then its average will be zero.
Let us see how the data looks like
```{python}
drug1 = sleep[sleep["group"] == 1][["ID", "extra"]]
drug2 = sleep[sleep["group"] == 2][["ID", "extra"]]
combined = drug1.join(drug2.set_index("ID"), on="ID", lsuffix="_1", rsuffix="_2")
extra_mean = np.mean(combined.extra_1 - combined.extra_2)
print(f"The mean of the differences in extra is {extra_mean:0.4}")
```
What is the chance that we can see a mean as extreme as this one under the null
hypothesis? To find it out, we need to calculate the relevant $t$-statistic.
```{python}
s = np.std(combined.extra_1 - combined.extra_2, ddof=1)
df = len(combined) - 1
t_stat = (extra_mean - 0)/s * np.sqrt(len(combined))
p_val = stats.t.cdf(t_stat, df=df) * 2
print(f"t-stat = {t_stat:0.4}, p = {p_val:0.6}, df = {df}")
```
Under the null hypothesis, there is only a $0.28\%$ chance that a $t$-statistic
as extreme as this one can be found. We can therefore reject the null hypothesis.

This analysis can be done using a single function in Python
```{python}
res = stats.ttest_rel(combined.extra_1, combined.extra_2)
print(f"t-stat = {res.statistic:0.4}, p = {res.pvalue:0.6}, df={res.df:0}")
```

## $t$-test when variances are not equal
The generalisation of the $t$-test for equality of sample means when the sample
variances and sizes are not equal was developed by Welch [@welch1947]. The
theory behind the formulae is quite involved and explained in Welch's paper. We
merely quote the formulae here. The $t$-statistic is calculated as
$$
t = \frac{\bar{X}_1 - \bar{X}_2}{\sqrt{s_a^2 + s_b^2}},
$$
where
\begin{eqnarray*}
s_a &=& \frac{s_1}{n_1} \\
s_b &=& \frac{s_2}{n_2}
\end{eqnarray*}
Here $s_{1, 2}$ are sample variances and $n_{1,2}$ are sample sizes. The number
of degrees of freedom of the $t$ statistic are
$$
\nu = \frac{(s_a^2 + s_b^2)^2}{s_a^4/(n_1 - 1) + s_b^4/(n_2 - 1)}.
$$
Unlike the previous cases, we will not demonstrate the calculations but use the
R function instead. We will use the `ChickWeight` data and find out if the
weight gained by the chicks on Diet 1 is the same as that gained by chicks on
Diet 2. Before we can analyse the data, we must transform it in a form where
we can easily calculate the weight gain.
```{python}
import pandas as pd

chick = rds.data("ChickWeight")
start_time = chick[["Chick", "Time"]].groupby("Chick").min()
end_time = chick[["Chick", "Time"]].groupby("Chick").max()

start = pd.merge(left=chick,
                 right=start_time,
                 how="inner",
                 left_on=["Chick", "Time"],
                 right_on=["Chick", "Time"])
end = pd.merge(left=chick,
               right=end_time,
               how="inner",
               left_on=["Chick", "Time"],
               right_on=["Chick", "Time"])
combined = pd.merge(left=start[["Chick", "Diet", "weight"]], 
                    right=end[["Chick", "Diet", "weight"]], 
                    left_on=["Chick", "Diet"],
                    right_on=["Chick", "Diet"], 
                    how="inner")
combined["wtgain"] = combined["weight_y"] - combined["weight_x"]
wgdata = combined[["Chick", "Diet", "wtgain"]]
wgdata.head()
```
We check the mean weight gain with Diets 1 and 2 and also calculate the sample
variances.
```{python}
wgdata[["Diet", "wtgain"]].groupby("Diet").mean().head()
```
```{python}
wgdata[["Diet", "wtgain"]].groupby("Diet").var(ddof=1) # Sample variances
```
Interestingly, the F-test on variances indicates that the variances are all
equal.
```{python}
diets = wgdata["Diet"].unique()
for i in range(1, len(diets)):
    for j in range(i + 1, len(diets) + 1):
        X = wgdata[wgdata["Diet"] == i]["wtgain"]
        Y = wgdata[wgdata["Diet"] == j]["wtgain"]
        vr = X.var(ddof=1)/Y.var(ddof=1)
        pv = stats.f.cdf(vr, dfn=len(X), dfd=len(Y))
        print(f"Diet {i} and Diet {j}, p-value of F test is {pv:0.4}")
```

We, therefore, carry out the $t$-test once assuming equal variances and then
assuming that they are different.
```{python}
res = stats.ttest_ind(wgdata[wgdata["Diet"] == 1]["wtgain"],
                      wgdata[wgdata["Diet"] == 2]["wtgain"],
                      equal_var=True)
print(f"t-statistic = {res.statistic:0.4}, p-value={res.pvalue:0.6}, df={res.df}")
```

Unlike the `t.test` function, we do not get the confidence intervals in the 
result of the function. We will calculate it instead.
```{python}
X = wgdata[wgdata["Diet"] == 1]["wtgain"]
Y = wgdata[wgdata["Diet"] == 2]["wtgain"]

n1 = len(X)
n2 = len(Y)
n_p = n1 * n2/(n1 + n2)
df = n1 + n2 - 2

s1_sq = X.var(ddof=1)
s2_sq = Y.var(ddof=1)
sp_sq = ((n1 - 1)*s1_sq + (n2 - 1)*s2_sq)/(n1 + n2 - 2)

alpha = 0.05
lims = (X.mean() - Y.mean() - 
        np.array([1, -1]) * stats.t.ppf(alpha/2, df) * np.sqrt(sp_sq/n_p))
print(f"The 95% confidence interval is {lims}")
```
When the variances are assumed equal, the confidence interval is entirely on
the left hand side of zero and we can conclude that the weight gain due Diet 1
is less than that due to Diet 2.

```{python}
res = stats.ttest_ind(wgdata[wgdata["Diet"] == 1]["wtgain"],
                      wgdata[wgdata["Diet"] == 2]["wtgain"],
                      equal_var=False)
print(f"t-statistic = {res.statistic:0.4}, p-value={res.pvalue:0.6}, df={res.df}")
```

Once again, we calculate the confidence interval on our own.
```{python}
n1 = len(X)
sa = X.var(ddof=1)/n1
n2 = len(Y)
sb = Y.var(ddof=1)/n2
df = (sa + sb)**2/(sa**2/(n1 - 1) + sb**2/(n2 - 1))

alpha = 0.05
lim = (X.mean() - Y.mean() + 
       np.array([1, -1]) * stats.t.ppf(alpha/2, df) * np.sqrt(sa + sb))

print(f"The 95% confidence interval is {lim}")
```
On the other hand, the $p$-value and the confidence interval suggest that we
cannot be so sure if we assume that the variances are unequal. Also notice that
in this case Welch's test was carried out instead of the standard $t$-test.

### An investigation into the variances of weight gain
It was a tad surprising that the F-test for comparing variances declared that
the all variances are likely to be same. Let us compare the density of the
data after centering all of them on the origin.
```{python}
X1 = (wgdata[wgdata["Diet"] == 1]["wtgain"] - 
      wgdata[wgdata["Diet"] == 1]["wtgain"].mean())
X2 = (wgdata[wgdata["Diet"] == 2]["wtgain"] - 
      wgdata[wgdata["Diet"] == 2]["wtgain"].mean())
X3 = (wgdata[wgdata["Diet"] == 3]["wtgain"] - 
      wgdata[wgdata["Diet"] == 3]["wtgain"].mean())
X4 = (wgdata[wgdata["Diet"] == 4]["wtgain"] - 
      wgdata[wgdata["Diet"] == 4]["wtgain"].mean())

kde1 = stats.gaussian_kde(X1)
kde2 = stats.gaussian_kde(X2)
kde3 = stats.gaussian_kde(X3)
kde4 = stats.gaussian_kde(X4)
X = np.linspace(start=-250, stop=250, num=100)

plt.plot(X, kde1(X), label="Diet 1")
plt.plot(X, kde2(X), label="Diet 2")
plt.plot(X, kde3(X), label="Diet 3")
plt.plot(X, kde4(X), label="Diet 4")
plt.legend(loc="upper left")
_ = plt.title("Centred distributions of weight gain")
```

