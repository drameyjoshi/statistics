{{< include macros.qmd >}}
# Population and Samples

A population is characterised by a cumulative probability distribution. A sample
is a subset of the population that we observe and measure. Measurements of the
sample are used to infer the parameters of the population. We will illustreate
this idea with the simplest of measurements - the mean.

## Sample mean {#sec-c1s1}
Consider a population with an infinite number of tosses of a fair coin. We draw
a random sample of ten such and find its mean.
```{r}
set.seed(12111842)
s1 <- rbinom(n=10, size=1, prob=0.5)
cat("Sample mean = ", mean(s1))
```
The mean of the sample is not $0.5$ although the population consists of tosses
of a fair coin. We next repeat the procedure a large number of times.
```{r}
n.sims <- 100
sample.size <- 10
S <- matrix(rbinom(n=n.sims * sample.size, size=1, prob=0.5), 
            nrow=sample.size)
sample.means <- colMeans(S)
hist(sample.means, xlab='Sample mean', main='Histogram of sample means')
```
We see that there is a wide variation in the sample means although a majority
of the times we do get the value $0.5$ appropriate for the tosses of a fair coin.

The sample mean is also a random variable and it has a distribution. It is 
called the _sampling distribution of mean_.

We can carry out this procedure for an infinite population of standard normal
variates.
```{r}
S <- matrix(rnorm(n=n.sims * sample.size), nrow=sample.size)
sample.means <- colMeans(S)
X <- seq(from=-3, to=3, length.out=100)
hist(sample.means,
     xlab="Sample mean",
     main="Sampling from a standard normal distribution",
     prob=TRUE,
     xlim=range(X))
lines(X, dnorm(X))
legend("topleft", 
       legend=c("Population density"), 
       col=c("black"), 
       cex=0.7, 
       lty=1,
       bty="n")
```

We observe that the sampling distribution of mean has a much lesser spread 
compared to the distribution of the population. This is to be expected as each
sample mean tries to come closer to the true centre of the population. We can
easily show that

::: {#prp-c1p1}
If a population has mean $\mu$ and variance $\sigma^2$ then the sampling 
distribution of mean has mean $\mu$ and variance $\sigma^2/n$, where $n$ is the
sample size.
:::

::: {.proof}
The sample mean is
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i,
$$
so that
$$
E(\bar{x}) = \frac{1}{n}\sum_{i=1}^n E(x_i) = \frac{1}{n}\sum_{i=1}^n \mu = \mu,
$$
and
$$
\var(\bar{x}) = \frac{1}{n^2}\var\left(\sum_{i=1}^n x_i\right).
$$
Here we used the fact that $\\var(aX) = a^2\var(X)$. Since 
$\var(X_1 + \cdots + X_n) = n\var{X}$, we have
$$
\var(\bar{x}) = \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n}.
$$
:::

The standard deviation of the sampling distribution is called its _standard
error_.

The histograms of the means of samples drawn from Bernoulli and standard normal
distibutions seem to be peaked around the population mean and tapering away from
it. The central limit theorem (see @wasserman2013all) guarantees that if $X_1,
\ldots, X_n$ are random variables drawn independently from a population with
mean $\mu$ and variance $\sigma^2 < \infty$ then 
$$
Z_n = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}
$$
is a standard normal random variable as $n \rightarrow \infty$. In other words, 
the random variable $\bar{X}$ has a normal distribution of mean $\mu$ and 
variance $\sigma^2/n$.

## Sample variance {#sec1-s2}
If $X_1, \ldots, X_n$ are a sample drawn from a population, its sample variance
is
$$
s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2.
$$
Being a quantity computed from the sample, $s^2$ is also a random variable. 
There are two results known about the distribution of sample variance:

- If $X_1, \ldots, X_n$ are such that $X_i \sim \dNorm(\mu, \sigma^2)$ then
$$
(n - 1)\frac{s^2}{\sigma^2} \sim \chi^2(n - 1).
$$
A proof of this statement can be found in [The Book of Statistical Proofs](
https://statproofbook.github.io/P/norm-chi2). Another proof is available in
an [Eberly College of Science](https://online.stat.psu.edu/stat414/lesson/26/26.3)
course.

- If $X_1, \ldots, X_n$ are drawn from a distibution with variance $\sigma^2$
and kurtosis $\kappa$ then 
$$
\frac{s^2}{\sigma^2} \sim \frac{\chi^2(m)}{m},
$$
where
$$
m = \frac{2n}{\kappa - (n - 3)/(n - 1)}.
$$
This fact is mentioned in an answer to a question on [Stack Exchange](https://stats.stackexchange.com/questions/316714/sampling-distribution-of-sample-variance-of-non-normal-iid-r-v-s)
in which the paper @ONeill02102014 is referred.

We will now show that

::: {#prp-c1p2}
$E(s^2) = \sigma^2$.
:::

::: {.proof}
Consider, $(n-1)S_n^2 = \sum_{i=1}^n(X_i - \bar{X}_n)^2$ so that
\begin{equation}\label{e2}
(n-1)E(S_n^2) = \sum_{i=1}^n\left(E(X_i^2) - 2E(X_i\bar{X}_n) + E(\bar{X}_n^2)\right)
\end{equation}
We first consider the term $E(X_i\bar{X}_n)$. It is
\begin{eqnarray*}
E(X_i\bar{X}_n) &=& E\left(X_i \sum_{j=1}^n\frac{X_j}{n}\right) \\
nE(X_i\bar{X}_n) &=& \sum_{j=1, j\ne i}^n E(X_iX_j) + E(X_i^2) \\
nE(X_i\bar{X}_n) &=& (n-1)\mu^2 + \mu^2 + \sigma^2 \\
E(X_i\bar{X}_n) &=& \mu^2 + \frac{\sigma^2}{n},
\end{eqnarray*}
as $X_i$ are iid. 

From @prp-c1p1, 
$$
E(\bar{X}_n^2) = \var{\bar{X}_n} - (E(\bar{X}_n))^2 = \frac{\sigma^2}{n} - \mu^2
$$
so that we have
$$
(n-1)E(S_n^2) = \sum_{i=1}^n\left(E(X_i^2) - 2\mu^2 - \frac{2\sigma^2}{n}
+ \frac{\sigma^2}{n} + \mu^2\right).
$$
The result follows from the fact that $E(X_i^2) = \mu^2 + \sigma^2$.
:::

## Convergence of mean {#sec1-s3}
If we toss a fair coin a large number of times we expect that eventually the
number of heads will be almost the same as the number of tails. However, the 
way this equality emerges is quite unlike the way a mathematical series 
converges. In the experiment with coins, let us calculate the empirical 
probability of getting a head as the number of tosses increases. It is 
interesting to observe that different experiments will converge to the expected
value of $0.5$ in different ways. If $X_n$ denotes the outcome of the $n$th toss
and if we let $X_n = 1$ if the coin lands on head and $X_n = 0$ if it lands on
tail then the probability of the number of times we get a head is also equal to
the sample mean $\bar{X}_n$. We treat this experiment as a way to determine the
sample mean.
```{r}
n.sims <- 1000
exp1 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims
exp2 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims
exp3 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims

plot(1:n.sims, exp1, xlab="# tosses", ylab=expression(bar(X)[n]), 
     col="green", type="l", main="Convergence of probability")
lines(1:n.sims, exp2, col="red", type="l")
lines(1:n.sims, exp3, col="blue", type="l")
abline(h=0.5, col="black", lty=2)
legend("topright",
       legend=c("Experiment 1", "Experiment 2", "Experiment 3"),
       col=c("green", "red", "blue"),
       lty=1,
       bty="n",
       cex=0.7)
```
Eventually, the sample means calculated in all experiments tends to be close to
$0.5$ but each one approaches it differently. This type of convergence is called
'convergence in probability'. The law of large number assures us that $\bar{X}_n$
converges to $\mu$ in probability.

## Confidence intervals of means {#sec1-s4}
We saw in section @sec-c1s1 that the sample mean is itself a random variable 
and has a distribution. Therefore, it is not usually helpful to quote just the
sample mean. We need some way to indicate how far the population mean is from
the sample mean.

The sampling distribution of means is a gaussian distribution with parameters
$\mu, \sigma^2/n$, where $n$ is the size of the sample. Therefore, the 
probability of getting a sample mean in the range $(\mu - \sigma/\sqrt{n}, \mu
+ \sigma/\sqrt{n})$ is close to $68\%$. Similarly, the probability of getting
an $\bar{X}_n$ in the range $(\mu - 2\sigma/\sqrt{n}, \mu + 2\sigma/\sqrt{n})$
is close to $95\%$ or the probability of $\bar{X}_n \le \mu - 2\sigma/\sqrt{n}$
or $\bar{X}_n \ge \mu + 2\sigma/\sqrt{n}$ is $5\%$. Now the event
$$
\mu - 2\frac{\sigma}{\sqrt{n}} \le \bar{X}_n \le \mu + 2\frac{\sigma}{\sqrt{n}}
$$
is equivalent to
$$
-2\frac{\sigma}{\sqrt{n}} \le \bar{X}_n - \mu \le 2\frac{\sigma}{\sqrt{n}}
$$
which is same as
$$
-2\frac{\sigma}{\sqrt{n}} \le -\bar{X}_n + \mu \le 2\frac{\sigma}{\sqrt{n}}
$$
or
$$
\bar{X}_n -2\frac{\sigma}{\sqrt{n}} \le \mu \le 
\bar{X}_n + 2\frac{\sigma}{\sqrt{n}}.
$$
Thus, the probability of $\bar{X}_n$ lying in an interval around $\mu$ is the
same as the probability of $\mu$ lying in the interval of same size around
$\bar{X}_n$. A statement about the complement of this event is that the
probability of $\bar{X}_n$ lying outside an interval around $\mu$ is the same
as the probability of $\mu$ lying outside the interval of the same size around
$\bar{X}_n$.

Note that this equivalence is as a result of the symmetry of the gaussian
distribution and it should not be applied to other distributions without
considerations of its shape.

We can summarise this section by stating that for a data set $X_1, \ldots, X_n$,
the probability of $\mu \in (\bar{X}_n - k\sigma/\sqrt{n}, \bar{X}_n + 
k\sigma/\sqrt{n})$ is `1 - 2 * pnorm(-k)` for an positive integer $k$.


## Confidence intervals in practice {#sec1-s5}
The confidence interval derived in the previous section depends on the 
distribution of the sample means. The central limit theorem guarantees that it
will be gaussian in the limit of the sample size going to infinity. In practice,
it means that when the sample sizes are large enough, the sampling distribution
of means looks like a gaussian. But the sample size at which it starts happening
depends on the distribution and its parameters from which the data are sampled. 
We will illustrate this point with the example of a binomial distribution.
```{r, fig.width=6, fig.height=12}
p1 <- 0.5
p2 <- 0.9
sample.size <- 20
n.samples <- 100
means.1 <- colMeans(matrix(rbinom(n=sample.size*n.samples, size=1, p=p1), 
                           nrow=sample.size))
means.2 <- colMeans(matrix(rbinom(n=sample.size*n.samples, size=1, p=p2), 
                           nrow=sample.size))
mean.means.1 <- p1
mean.means.2 <- p2
sd.means.1 <- sqrt(means.1 * (1 - means.1)/sample.size)
sd.means.2 <- sqrt(means.2 * (1 - means.2)/sample.size)
X1 <- seq(from=mean.means.1 - 0.5, to=mean.means.1 + 0.5, length.out=100)
X2 <- seq(from=mean.means.2 - 0.5, to=mean.means.2 + 0.5, length.out=100)
N1 <- dnorm(X1, mean=mean.means.1, 
            sd=sqrt(mean.means.1 * (1 - mean.means.1)/sample.size))
N2 <- dnorm(X2, mean=mean.means.2, 
            sd=sqrt(mean.means.2 * (1 - mean.means.2)/sample.size))
oldpar <- par(mfrow=c(2, 1))
plot(X1, N1, lty=2, main=paste("p =", p1), type="l")
lines(density(means.1))
legend("topright",
       legend=c("Density", "Gaussian"),
       lty = c(1, 2),
       bty = "n")
plot(density(means.2), main=paste("p =", p2), type="l")
lines(X2, N2, lty=2)
legend("topright",
       legend=c("Density", "Gaussian"),
       lty = c(1, 2),
       bty = "n")
```

Clearly, the density is much closer to being an gaussian when $p = 0.5$ than
when $p = 0.8$. Therefore, building a confidence interval assuming that the
sampling distribution of means is gaussian is grossly wrong in the latter case.
In the former case, the parameters of the sampling distribution can be taken as 
$\mu = \hat{p} = \bar{X}_n$ and $\sigma^2 = \hat{p}(1 - \hat{p})$. An interval
calculated using these parameters is called the _Wald interval_. For the second
sample, the Wald interval is not appropriate as we show in the simulation below
for the case $p = 0.9$.
```{r}
samples <- matrix(data=rbinom(n=sample.size*n.samples, size=1, p=p2),
                  nrow=sample.size)
sample.means <- colMeans(samples)
sample.sd <- sqrt(sample.means * (1 - sample.means)/sample.size)
# We are looking for a 95% interval, so the limits are +/- 2 standard
# deviations from the mean.
k <- 2
lower.lim <- sample.means - k * sample.sd
upper.lim <- sample.means + k * sample.sd
mu <- p2
# Find out if mu lies in these limits for each sample.
result <- (lower.lim <= mu) & (mu <= upper.lim)
cat(paste("The confidence interval captured parametric mean",
          round(sum(result)/length(result) * 100, 2),
          "% of times.\n"))
cat("We expected it to capture 95% of times.\n")
```

This is not a pathological example of the failure of Wald interval. Often times
we want to sample to estimate the parametric probability of a rare event. 
Clearly, in this case, the Wald interval misses to capture the parametric 
probability far too many times. A simple trick due to Agresti and Coull seems to
work wonders. It consists of augumenting every sample with two successes and two
failures.
```{r}
# Quantities related to Agreti-Coull interval will be prefixed with 'ac.'.
ac.samples <- rbind(samples,
                    rep(1, n.samples),
                    rep(1, n.samples),
                    rep(0, n.samples),
                    rep(0, n.samples))
ac.sample.means <- colMeans(ac.samples)
ac.sd <- sqrt(ac.sample.means * (1 - ac.sample.means)/sqrt(sample.size))
ac.lower.lim <- ac.sample.means - k * ac.sd
ac.upper.lim <- ac.sample.means + k * ac.sd
ac.result <- (ac.lower.lim <= mu) & (mu <= ac.upper.lim)
cat(paste("The confidence interval captured parametric mean",
          round(sum(ac.result)/length(ac.result) * 100, 2),
          "% of times.\n"))
cat("We expected it to capture 95% of times.\n")
```

However, this adjustment comes at a cost. Let us compare the sample means with
and without the Agresti-Coull adjustment.
```{r}
summary(sample.means)
summary(ac.sample.means)
```

The sample means of the Agresit-Coull augmented data are shifted lower than the
observed sample means. However, one must note that the Agresti-Coull adjustment
is done only to find good confidence intervals. The reported sample mean should 
be the one calculated prior to augmentation of two successes and two failures.

There are other methods of computing the confidence intervals for a binomial
distribution. An early method by Clopper and Pearson [@ClopperPearson1934] is called
and _exact_ method because it does not use the central limit theorem to 
approximate the sampling distribution of means. Rather, it derives them from the
properties of binomial distribution alone. The [Wikipedia page](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#)
on binomial proportions describes a few more ways to compute the confidence
intervals.
