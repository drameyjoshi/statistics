{{< include macros.qmd >}}
# Population and Samples

A population is characterised by a cumulative probability distribution. A sample
is a subset of the population that we observe and measure. Measurements of the
sample are used to infer the parameters of the population. We will illustreate
this idea with the simplest of measurements - the mean.

## Sample mean {#sec-c1s1}
Consider a population with an infinite number of tosses of a fair coin. We draw
a random sample of ten such and find its mean.
```{r}
set.seed(12111842)
s1 <- rbinom(n=10, size=1, prob=0.5)
cat("Sample mean = ", mean(s1))
```
The mean of the sample is not $0.5$ although the population consists of tosses
of a fair coin. We next repeat the procedure a large number of times.
```{r}
n.sims <- 100
sample.size <- 10
S <- matrix(rbinom(n=n.sims * sample.size, size=1, prob=0.5), nrow=sample.size)
sample.means <- colMeans(S)
hist(sample.means, xlab='Sample mean', main='Histogram of sample means')
```
We see that there is a wide variation in the sample means although a majority
of the times we do get the value $0.5$ appropriate for the tosses of a fair coin.

The sample mean is also a random variable and it has a distribution. It is 
called the _sampling distribution of mean_.

We can carry out this procedure for an infinite population of standard normal
variates.
```{r}
S <- matrix(rnorm(n=n.sims * sample.size), nrow=sample.size)
sample.means <- colMeans(S)
X <- seq(from=-3, to=3, length.out=100)
hist(sample.means,
     xlab="Sample mean",
     main="Sampling from a standard normal distribution",
     prob=TRUE,
     xlim=range(X))
lines(X, dnorm(X))
legend("topleft", 
       legend=c("Population density"), 
       col=c("black"), 
       cex=0.7, 
       lty=1,
       bty="n")
```

We observe that the sampling distribution of mean has a much lesser spread 
compared to the distribution of the population. This is to be expected as each
sample mean tries to come closer to the true centre of the population. We can
easily show that

::: {#prp-c1p1}
If a population has mean $\mu$ and variance $\sigma^2$ then the sampling 
distribution of mean has mean $\mu$ and variance $\sigma^2/n$, where $n$ is the
sample size.
:::

::: {.proof}
The sample mean is
$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i,
$$
so that
$$
E(\bar{x}) = \frac{1}{n}\sum_{i=1}^n E(x_i) = \frac{1}{n}\sum_{i=1}^n \mu = \mu,
$$
and
$$
\var(\bar{x}) = \frac{1}{n^2}\var\left(\sum_{i=1}^n x_i\right).
$$
Here we used the fact that $\\var(aX) = a^2\var(X)$. Since 
$\var(X_1 + \cdots + X_n) = n\var{X}$, we have
$$
\var(\bar{x}) = \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n}.
$$
:::

The standard deviation of the sampling distribution is called its _standard
error_.

The histograms of the means of samples drawn from Bernoulli and standard normal
distibutions seem to be peaked around the population mean and tapering away from
it. The central limit theorem (see @wasserman2013all) guarantees that if $X_1,
\ldots, X_n$ are random variables drawn independently from a population with
mean $\mu$ and variance $\sigma^2 < \infty$ then 
$$
Z_n = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}
$$
is a standard normal random variable. In other words, the random variable 
$\bar{X}$ has a normal distribution of mean $\mu$ and variance $\sigma^2/n$.

## Sample variance {#sec1-c1s2}
If $X_1, \ldots, X_n$ are a sample drawn from a population, its sample variance
is
$$
s^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \bar{X})^2.
$$
Being a quantity computed from the sample, $s^2$ is also a random variable. 
There are two results known about the distribution of sample variance:

- If $X_1, \ldots, X_n$ are such that $X_i \sim \dNorm(\mu, \sigma^2)$ then
$$
(n - 1)\frac{s^2}{\sigma^2} \sim \chi^2(n - 1).
$$
A proof of this statement can be found in [The Book of Statistical Proofs](
https://statproofbook.github.io/P/norm-chi2). Another proof is available in
an [Eberly College of Science](https://online.stat.psu.edu/stat414/lesson/26/26.3)
course.

- If $X_1, \ldots, X_n$ are drawn from a distibution with variance $\sigma^2$
and kurtosis $\kappa$ then 
$$
\frac{s^2}{\sigma^2} \sim \frac{\chi^2(m)}{m},
$$
where
$$
m = \frac{2n}{\kappa - (n - 3)/(n - 1)}.
$$
This fact is mentioned in an answer to a question on [Stack Exchange](https://stats.stackexchange.com/questions/316714/sampling-distribution-of-sample-variance-of-non-normal-iid-r-v-s)
in which the paper @ONeill02102014 is referred.

We will now show that

::: {#prp-c1p2}
$E(s^2) = \sigma^2$.
:::

::: {.proof}
Consider, $(n-1)S_n^2 = \sum_{i=1}^n(X_i - \bar{X}_n)^2$ so that
\begin{equation}\label{e2}
(n-1)E(S_n^2) = \sum_{i=1}^n\left(E(X_i^2) - 2E(X_i\bar{X}_n) + E(\bar{X}_n^2)\right)
\end{equation}
We first consider the term $E(X_i\bar{X}_n)$. It is
\begin{eqnarray*}
E(X_i\bar{X}_n) &=& E\left(X_i \sum_{j=1}^n\frac{X_j}{n}\right) \\
nE(X_i\bar{X}_n) &=& \sum_{j=1, j\ne i}^n E(X_iX_j) + E(X_i^2) \\
nE(X_i\bar{X}_n) &=& (n-1)\mu^2 + \mu^2 + \sigma^2 \\
E(X_i\bar{X}_n) &=& \mu^2 + \frac{\sigma^2}{n},
\end{eqnarray*}
as $X_i$ are iid. 

From proposition @prp-c1p1, 
$$
E(\bar{X}_n^2) = \var{\bar{X}_n} - (E(\bar{X}_n))^2 = \frac{\sigma^2}{n} - \mu^2
$$
so that we have
$$
(n-1)E(S_n^2) = \sum_{i=1}^n\left(E(X_i^2) - 2\mu^2 - \frac{2\sigma^2}{n}
+ \frac{\sigma^2}{n} + \mu^2\right).
$$
The result follows from the fact that $E(X_i^2) = \mu^2 + \sigma^2$.
:::

## Convergence of mean {#sec1-c1s3}
If we toss a fair coin a large number of times we expect that eventually the
number of heads will be almost the same as the number of tails. However, the 
way this equality emerges is quite unlike the way a mathematical series 
converges. In the experiment with coins, let us calculate the empirical 
probability of getting a head as the number of tosses increases. It is 
interesting to observe that different experiments will converge to the expected
value of $0.5$ in different ways. If $X_n$ denotes the outcome of the $n$th toss
and if we let $X_n = 1$ if the coin lands on head and $X_n = 0$ if it lands on
tail then the probability of the number of times we get a head is also equal to
the sample mean $\bar{X}_n$. We treat this experiment as a way to determine the
sample mean.
```{r}
n.sims <- 1000
exp1 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims
exp2 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims
exp3 <- cumsum(sample(0:1, n.sims, replace=TRUE))/1:n.sims

plot(1:n.sims, exp1, xlab="# tosses", ylab=expression(bar(X)[n]), 
     col="green", type="l", main="Convergence of probability")
lines(1:n.sims, exp2, col="red", type="l")
lines(1:n.sims, exp3, col="blue", type="l")
abline(h=0.5, col="black", lty=2)
legend("topright",
       legend=c("Experiment 1", "Experiment 2", "Experiment 3"),
       col=c("green", "red", "blue"),
       lty=1,
       bty="n")
```
Eventually, the sample means calculated in all experiments tends to be close to
$0.5$ but each one approaches it differently. This type of convergence is called
'convergence in probability'. The law of large number assures us that $\bar{X}_n$
converges to $\mu$ in probability.

## Confidence intervals of means {#sec1-c1s4}
We saw in section @sec-c1s1 that the sample mean is itself a random variable 
and has a distribution. Therefore, it is not usually helpful to quote just the
sample mean. We need some way to indicate how far the population mean is from
the sample mean.

The sampling distribution of means is a gaussian distribution with parameters
$\mu, \sigma^2/n$, where $n$ is the size of the sample. Therefore, the 
probability of getting a sample mean in the range $(\mu - \sigma/\sqrt{n}, \mu
+ \sigma/\sqrt{n})$ is close to $68\%$. Similarly, the probability of getting
an $\bar{X}_n$ in the range $(\mu - 2\sigma/\sqrt{n}, \mu + 2\sigma/\sqrt{n})$
is close to $95\%$ or the probability of $\bar{X}_n \le \mu - 2\sigma/\sqrt{n}$
or $\bar{X}_n \ge \mu + 2\sigma/\sqrt{n}$ is $5\%$. Now the event
$$
\mu - 2\frac{\sigma}{\sqrt{n}} \le \bar{X}_n \le \mu + 2\frac{\sigma}{\sqrt{n}}
$$
is equivalent to
$$
-2\frac{\sigma}{\sqrt{n}} \le \bar{X}_n - \mu \le 2\frac{\sigma}{\sqrt{n}}
$$
which is same as
$$
-2\frac{\sigma}{\sqrt{n}} \le -\bar{X}_n + \mu \le 2\frac{\sigma}{\sqrt{n}}
$$
or
$$
\bar{X}_n -2\frac{\sigma}{\sqrt{n}} \le \mu \le 
\bar{X}_n + 2\frac{\sigma}{\sqrt{n}}.
$$
Thus, the probability of $\bar{X}_n$ lying in an interval around $\mu$ is the
same as the probability of $\mu$ lying in the interval of same size around
$\bar{X}_n$. A statement about the complement of this event is that the
probability of $\bar{X}_n$ lying outside an interval around $\mu$ is the same
as the probability of $\mu$ lying outside the interval of the same size around
$\bar{X}_n$.

Note that this equivalence is as a result of the symmetry of the gaussian
distribution and it should not be applied to other distributions without
considerations of its shape.

We can summarise this section by stating that for a data set $X_1, \ldots, X_n$,
the probability of $\mu \in (\bar{X}_n - k\sigma/\sqrt{n}, \bar{X}_n + 
k\sigma/\sqrt{n})$ is `1 - 2 * pnorm(-k)` for an positive integer $k$.

