\documentclass{article}
\include{common}
\begin{document}
\begin{defn}\label{c2d1}
Let $\Omega$ be a sample space and $\mathcal{F}$ be a $\sigma$-algebra on 
$\Omega$.  Then members of $\mathcal{F}$ are called measurable sets.
\end{defn}

If a set $A$ is in $\sigma$-algebras $\mathcal{F}_1$ and $\mathcal{F}_2$ then it
is also in the $\sigma$-algebra $\mathcal{F}_1 \cap \mathcal{F}_2$. The smallest
$\sigma$-algebra containing $A$ is called the $\sigma$-algebra generated by $A$
and is denoted by $\sigma(A)$. An example of such a $\sigma$-algebra is the one
generated by open intervals of $\sor$. It is called the Borel $\sigma$-algebra.

\begin{defn}\label{c2d2}
Let $(\Omega, \mathcal{F})$ be a measurable space. A function $m:\Omega
\rightarrow [0, \infty]$ is called a measure if
\begin{itemize}
\item $m(\varnothing) = 0$,
\item If $A_1, A_2, \ldots$ are pair-wise disjoint members of $\mathcal{F}$ then
\[
m\left(\bigcup_{k \ge 1}A_k\right) = \sum_{k \ge 1}m(A_k).
\]
\end{itemize}
\end{defn}
If the range of $m$ is $[0, 1]$ then it is called the probability measure and is
denoted by $P$. The triple $(\Omega, \mathcal{F}, m)$ is called a measure space
while $(\Omega, \mathcal{F}, m)$ is called a probability space. For a probability
space, $P(\Omega) = 1$. Since  $A \cup A^c = \Omega$ is a disjoing union of 
members of $\mathcal{F}$, it follows that $P(A) + P(A^c) = P(\Omega) = 1$.

\begin{defn}\label{c2d3}
Let $(X, \Sigma_X)$ and $(Y, \Sigma_Y)$ be measurable spaces. A map $f: X 
\rightarrow Y$ is said to be measurable if $f^{-1}(B) \in \Sigma_X$ for all
$B \in \Sigma_Y$.
\end{defn}

\begin{defn}\label{c2d4}
Let $(\Omega, \mathcal{F}, P)$ be a probability space then a random variable
is a measurable map $X: \Omega \rightarrow \sor$, where it is understood that
the $\sigma$-algebra on $\sor$ is the Borel $\sigma$-algebra. In other words,
if for every open interval $(a, b)$, $P^{-1}((a, b)) \in \mathcal{F}$.
\end{defn}

Examples of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ be the sample space of a coin-toss. Then $X(H)=1,
X(T)=0$ is a random variable.
\item If $\Omega = \{H, T\}$ and a person wins $20$ quid if the coin lands head
or loses $10$ if the coin lands tail then $X(H) = 20, X(T) = -10$ is also a 
random variable. Thus, a random variable associates a real number to every 
outcome of an experiment.
\item If $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ then $X(\omega) = x, Y(
\omega) = y, R(\omega) = \sqrt{x^2 + y^2}, L(\omega) = |x + y|$ are some examples
of random variables.
\end{enumerate}

\begin{defn}\label{c2d5}
Let $(\Omega, \mathcal{F}, P)$ be a probability space and $X$ be a random variable
on it. If $A$ is a measurable subset of $\sor$ then $P(A) = P(X^{-1}(A))$.
\end{defn}

Thus, $P$ is also a probability measure on $(\sor, \mathcal{B}, m)$ where 
$\mathcal{B}$ is the Borel $\sigma$-algebra on $\sor$ and $m$ is a Lebesgue 
measure. (That is, if $I = (a, b)$ then $m(I) = b - a$.)

Examples of probability of random variables:
\begin{enumerate}
\item Let $\Omega = \{H, T\}$ such that $P(H) = 0.3, P(T) = 0.7$. If $X(H)=1,
X(T)=0$ then $P(1) = 0.3, P(0) = 0.7$.
\item If $\Omega = \{HH, HT, TH, TT\}$ be the sample space of two tosses of a
coin for which the probability of getting a head is $p$. If $P(HH) = 2, P(HT) = 
P(TH) = 1, P(TT) = 0$ then $P(2) = p^2, P(1) = 2p(1-p),P(0) = (1 - p)^2$.
\item Let $\Omega = \{(x, y) \;|\; x^2 + y^2 \le 1\}$ denote a dart-board whose
payoff is denoted by 
\[
X(\omega) = \begin{cases}
0 \text{ if } \sqrt{x^2 + y^2} > 1/2 \\
1 \text{ if } \sqrt{x^2 + y^2} \le 1/2,
\end{cases}
\]
where $\omega$ is the point where the dart lands and its coordinates are 
$(x, y)$, then $P(X = 0) = 1/2$ and $P(X = 1) = 1/2$.
\end{enumerate}

\begin{defn}\label{c2d6}
Let $X$ be a random variable on a probability space $(\Omega, \mathcal{F}, P)$.
Its cumulative distribution function $F_X: \sor \rightarrow [0, 1]$ is defined
as $F_X(x) = P(X \le x)$.
\end{defn}

Unless otherwise stated, we will always assume that all random variables are 
defined over a probability space $(\Omega, \mathcal{F}, P)$.
\begin{prop}\label{c2p1}
Let $X$ be a random variable with cfd $F$. If $I$ is the interval $(a, b)$ then
$P(I) = F(b) - F(a)$.
\end{prop}
\begin{proof}
$F(b) = P(X \le b) = P(\omega \in X^{-1}((-\infty, b]))$. Since $(-\infty, a]
\cup (a, b] = (-\infty, b]$ and the union is of disjoint sets,
\[
P((-\infty, A]) + P((a, b]) = P((-\infty, b]) \Rightarrow F(a) + P((a, b]) 
= F(b).
\]
Now $P(I) = P((a, b)) = P((a, b])$ because $(a, b] = (a, b) \cup \{b\}$ and 
$P(\{b\}) = 0$. Therefore, $P(I) = F(b) - F(a)$.
\end{proof}

\begin{thm}\label{c2t1}
If $X$ and $Y$ are random variables with cdfs $F$ and $G$ then $F(x) = G(x)
\Rightarrow P(X \in A) = P(Y \in A)$ for all measurable sets $A$. 
\end{thm}
\begin{proof}
Here $A$ is a measurable subset of $\mathcal{B}$, the Borel $\sigma$-algebra on
$\sor$. It can be written as a countable union of open intervals or their 
complements. Any countable union of sets can be written as a countable union of
disjoint sets. Therefore, any $A \in \mathcal{B}$ can be written as a countable
union of pair-wise disjoint open intervals.

Since $F(x) = G(x)$, by proposition \ref{c2p1}, $P(X \in A) = P(Y \in A)$ for all
open intervals $A$. Since any measurable set can be written as a countable union
of open intervals or their complements and for each of these sets, $A$, 
$P(X \in A) = P(Y \in A)$, the equality is also true for all measurable sets.
\end{proof}

\begin{thm}\label{c2t2}
Let $F$ be the cdf of a random variable $X$ then:
\begin{enumerate}
\item $x_1 < x_2 \Rightarrow F(x_1) \le F(x_2)$,
\item $F(x) \rightarrow 0$ as $x \rightarrow -\infty$ and $F(x) \rightarrow 1$
as $x \rightarrow \infty$ and
\item $F$ is right continuous for all $x \in \sor$.
\end{enumerate}
\end{thm}
\begin{proof}
$F(x_2) = P(X \le x_2) = P(\omega \in (-\infty, x_2]) = P(\omega \in (-\infty, x_1]
\cup (x_1, x_2]) = P(X \le x_1) + P((x_1, x_2]) \ge F(x_1)$.

As $x \rightarrow \infty$, the set $X \le x$ becomes all of $\sor$ so that 
$X^{-1}(\sor)$ is $\Omega$ and hence $P(X^{-1}(\sor)) = P(\Omega) = 1$. Likewise,
as $x \rightarrow -\infty$, the set $X \le x$ becomes $\varnothing$ for which
the probability is zero.

To prove right continuity at $x$, consider a monotonically decreasing sequence 
$\{y_n\}$ such that $y_n \rightarrow x$. If $A_n = (-\infty, y_n]$ then $A_1
\supseteq A_2 \supseteq \ldots$ and $\cap_{n \ge 1}A_n = A$; $A$ is a subset of
all $A_n$. Therefore,
\[
\lim_{n \rightarrow \infty}P(A_n) = P(A)
\]
that is, 
\[
\lim_{y_n \rightarrow x^+}F(y_n) = F(x),
\]
so that $F$ is right continuous.
\end{proof}
\begin{rem}
The same reasoning does not work from the left side. Suppose the sequence $\{y_n\}$
is monotonically increasing with limit $x$ and we consider the sets $A_n = (-\infty,
y_n]$. Then the union of these sets is $(-\infty, x)$ and not $(-\infty, x]$.
\end{rem}

\begin{defn}\label{c2d7}
A random variable whose range is a countable set is called a discrete random 
variable. If $X$ is a discrete random variable, its probability mass function,
also called pmf, is $f_X(x) = P(X = x)$.
\end{defn}

In terms of $f$, $F(x) = \sum_{x^\op \le x}P(x^\op)$.

\begin{defn}\label{c2d8}
A random variable whose range is uncountable is called a continuous random
variable. If $X$ is a continuous random variable, its density function $f$ is
defined as
\[
\int_a^b f(x)dx = P(a \le X \le b),
\]
where $F$ is its cdf.
\end{defn}

In terms of $f$,
\[
F(x) = \int_{-\infty}^x f(x)dx.
\]

\begin{prop}\label{c2p2}
Let $F$ be the cdf of a random variable $X$. Then:
\begin{enumerate}
\item $P(X=x) = F(x) - F(x^-)$, where $F(x^-) = \lim_{y \uparrow x}F(y)$.
\item $P(x < X \le y) = F(y) - F(x)$.
\item $P(X > x) = 1 - F(x)$.
\item If $X$ is continuous then $F(b) - F(a) = P(a < x < b) = P(a \le x < b) =
P(a < x \le b) = P(a \le x \le b)$.
\end{enumerate}
\end{prop}
\begin{proof}
Let $\{y_n\}$ be a monotone increasing sequence of numbers with limit $x$. Then
$P(X \le y_n) = P(A_n)$, where $A_n = (-\infty, y_n]$. The sets $\{A_n\}$ are 
also monotone increasing with union $A = (-\infty, x)$. Therefore,
\[
\lim_{n \rightarrow \infty}P(X \le y_n) = P(A).
\]
Since $(-\infty, x] = (-\infty, x) + \{x\}$, $P(X=x) = F(x) - F(x^-)$.

The set $A = \{x < X \le y\}$ can be written as $(-\infty, x] \cup A = 
(-\infty, y]$. Therefore, $P(A) = F(y) - F(x)$.

Since the set $X > x$ is a complement of the set $X \ge x$ or equivalently,
$x \le X$, $P(X > x) = 1 - P(x \le X) = 1 - F(x)$.

If $F$ is continuous then $F(x^-) = F(x) = F(x^+)$ so that $P(X=x) = 0$. Since
$(a, b) \cup {b} = (a, b]$, $P(X \in (a, b)) + P(X = b) = P(X \in (a, b])$ so 
that $P(X \in (a, b)) = P(X \in (a, b])$. Other relations can be similarly 
proved.
\end{proof}

\begin{defn}\label{c2d9}
Let $X$ be a random variable with cdf $F$. Then its quantile function $Q$ is 
defined as $Q(p) = \inf\{x \;|\; F(x) \ge p\}$, for $p \in [0, 1]$.
\end{defn}
\begin{rem}
The right continuity of $F$ makes $\inf\{x \;|\; F(x) \ge p\} = 
\inf\{x \;|\; F(x) > p\}$.
\end{rem}

\begin{defn}\label{c2d10}
Random variables $X$ and $Y$ are said to be equal in distribution if $F_X(x) = 
F_Y(x)$ for all $x \in \sor$. Equality in distribution is written as $X
\stackrel{d}{=} Y$.
\end{defn}
We will illustrate this tricky concept with an example. Consider a game between
two players which involves tossing a coin. Player `A' wins £1 if a coin lands
head and loses £1 if it lands tail. His opponent `B' loses £1 if the coin lands
head and wins £1 otherwise. The sample space is $\{H, T\}$. Let $X$ and $Y$ be 
the pay-off functions of the two players. Then $X(H) = 1, X(T) = -1, Y(H) = -1$
and $Y(T) = 1$. If the coin is fair, $F_X(-1) = 1/2, F_X(1) = 1, F_Y(-1) = 1/2,
F_Y(1) = 1/2$ although $Y = -X$. If the coin was unfair then we would not have
had this situation. For in that case, $F_X(-1) = 1 - p, F_X(1) = p, F_Y(-1) = p,
F_Y(1) = 1 - p$.

Examples of random variables.
\begin{enumerate}
\item $X \sim \delta_a$ if $P(X=a) = 1$ and $P(x) = 0$ for $x \ne 1$. Its cdf is
\begin{equation}\label{c2e1}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
1 & \text{ if } x \ge a.
\end{cases}
\end{equation}
This  reminds of `integral of Dirac delta function' being the `Heaviside step
function'.

\item Let $k > 1$ be an integer and let 
\begin{equation}\label{c2e2}
P(X=x) = f(x) = \begin{cases}
\frac{1}{k} & \text{ if } x = 1, 2, \ldots, k \\
0 & \text{ otherwise.}
\end{cases}
\end{equation}
the $X$ is called a uniform discrete distribution. All points $1, 2, \ldots, k$
have a uniform probability of $1/k$. Its cdf is
\begin{equation}\label{c2e3}
F(x) = \begin{cases}
0 \text{ if } x \le 0 \\
\frac{x}{k} \text{ if } x = 1, 2, \ldots, k \\
1 \text{ if } x > k.
\end{cases}
\end{equation}

\item $X \sim \dBer(p)$ if $X(0) = p, X(1) = 1 - p$. Its cdf if
\begin{equation}\label{c2e4}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
p & \text{ if } x \in [0, 1) \\
1 & \text{ if } x \ge 1.
\end{cases}
\end{equation}

\item $X \sim \dBin(n, p)$ is $X$ is the number of times a coin lands heads if
it is tossed $n$ times and if the probability of landing head is $p$. Its pmf
is 
\begin{equation}\label{c2e5}
P(X=x) = \binom{n}{x}p^x(1 - p)^{n-x}.
\end{equation}
Its cdf is
\begin{equation}\label{c2e6}
F(x) = \begin{cases}
0 & \text{ if } x < 0 \\
 & \text{ if } \sum_{k=0}^x\binom{n}{k}p^k(1 - p)^{n-k} \\
 & 1 \text{ if } x \ge n.
\end{cases}
\end{equation}

\item The random variable that counts the number of coin tosses before the first
head is called the geometric random variable. If the coin has a probability $p$
of landing head then $Y \sim \dGeo(p)$ and
\begin{equation}\label{c2e7}
f(X=x) = P(X=x) = (1 - p)^{x-1}p.
\end{equation}
Its cdf is
\begin{eqnarray}
F(x) &=& \sum_{k=1}^x (1-p)^{k-1}p \nonumber \\
 &=& p\sum_{k=0}^{x-1}(1 - p)^k \nonumber \\
 &=& p\frac{(1 - (1 - p)^x)}{1 - 1 + p} \nonumber \\
 &=& (1 - (1 - p)^x) \label{c2e8}
\end{eqnarray}

\item A Poisson random variable with parameter $\lambda$ is defined as 
\begin{equation}\label{c2e9}
f(x) = e^{-\lambda}\frac{\lambda^x}{x!}.
\end{equation}

\item We can also have a uniform, continuous random variable. Its density is
\begin{equation}\label{c2e10}
f(x) = \begin{cases}
=\frac{1}{b - a} & \text{ if } if x \in [a, b] \\
= 0 & \text{otherwise}.
\end{cases}
\end{equation}
Its cdf is
\begin{equation}\label{c2e11}
F(x) = \begin{cases}
0 & \text{ if } x < a \\
\frac{x - a}{b - a} & \text{ if } x \in [a, b] \\
1 & \text{ if } x > b.
\end{cases}
\end{equation}

\item $X \sim \dNor(\mu, \sigma^2)$ has a normal or gaussian random variable with 
parameters $\mu$ and $\sigma$ if its density is,
\begin{equation}\label{c2e12}
f(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x - \mu)^2}{\sigma^2}\right).
\end{equation}
Here $x \in \sor$. Its cdf is the error function,
\begin{equation}\label{c2e13}
F(x) = \frac{1}{2}\left(1 + \erf\left(\frac{x - \mu}{\sigma\sqrt{2}}\right)\right),
\end{equation}
where the error function is defined as 
\begin{equation}\label{c2e14}
\erf(x) = \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2}dt.
\end{equation}
A standard normal distribution has parameters $\mu = 0, \sigma = 1$. The density 
and cdf of a standard normal distribution are denoted by $\varphi$ and $\Phi$ 
respectively. A standard normal random variable is denoted by $Z$. Properties of
the normal random variable will be proved in later chapters after we introduce the
mean and the variance.

\item $X \sim \dExp(\beta)$ has an exponential density given by
\begin{equation}\label{c2e15}
f(x) = \frac{1}{\beta}\exp\left(-\frac{x}{\beta}\right), 
\end{equation}
if $x > 0$. Its cdf is 
\begin{equation}\label{c2e16}
F(x) = 1 - \exp\left(-\frac{x}{\beta}\right).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dGam(\alpha, \beta)$ if
\begin{equation}\label{c2e17}
f(x) = \frac{1}{\beta^\alpha\Gamma(\alpha)}x^{\alpha - 1}
\exp\left(-\frac{x}{\beta}\right),
\end{equation}
where
\begin{equation}\label{c2e18}
\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1}e^{-x}dx.
\end{equation}
Clearly,
\begin{equation}\label{c2e19}
\dExp(\beta) = \dGam(1, \beta).
\end{equation}

\item For $\alpha, \beta > 0$, $X \sim \dBet(\alpha, \beta)$ if
\begin{equation}\label{c2e20}
f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha, \beta)}
       x^{\alpha - 1}(1 - x)^{\beta - 1}
\end{equation}

\item $X \sim t(\nu)$ if
\begin{equation}\label{c2e21}
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	   \frac{1}{\left(1 + \frac{x^2}{\nu}\right)^{(\nu+1)/2}},
\end{equation}
for $x \in \sor$. As $\Gamma(1/2) = \sqrt{\pi}$, the density of $X \sim t(1)$ is
\begin{equation}\label{c2e22}
f(x) = \frac{1}{\pi}\frac{1}{1 + x^2}.
\end{equation}
This is called the Cauchy density. In proposition \ref{c2p5} we show that as 
$\nu \infty$ the $t$ distribution becomes the standard normal distribution.

\item $X \sim \chi^2(\nu)$ if $x > 0$ and
\begin{equation}\label{c2e23}
f(x) = \frac{1}{\Gamma(\nu/2) 2^{\nu/2}}x^{\nu/2 - 1}e^{-x/2}.
\end{equation}
\end{enumerate}

We prove a few propositions about the random variables introduced so far.
\begin{prop}\label{c2p3}
Let $X_1 \sim \dBin(n_1, p), X_2 \sim \dBin(n_2, p)$ then $X_1 + X_1 \sim
\dBin(n_1 + n_2, p)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. Then $Y = y, X_1 = x_1, X_2 = x_2$ iff $y = x_1 + x_2$ and
hence $P(Y = y) = P(X_1 = x_1, X_2 = y - x_1) = P(X_1 = x_1)P(X_2 = y - x_1)$
as $X_1$ and $X_2$ are independent. Thus,
\begin{eqnarray*}
f_Y(y) &=& \sum_{k=0}^y\binom{n_1}{k}p^{k}(1 - p)^{n_1 - k}\binom{n_2}{y - k}
p^{y - k}(1 - p)^{n_2 - y + k} \\
 &=& p^y(1 - p)^{n_1 + n_2 - y}
 \sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k}
\end{eqnarray*}
The number of ways to choose $y$ objects from a pool of $n_1 + n_2$ objects is
to choose $k$ objects from among $n_1$ and $y - k$ from among $n_2$ for each
$k = 0, \ldots y$. Therefore, by this combinatorial argument,
\[
\sum_{k=0}^y\binom{n_1}{k}\binom{n_2}{y - k} = \binom{n_1+n_2}{y},
\]
so that
\[
f_Y(y) = \binom{n_1+n_2}{y}p^y(1 - p)^{n_1 + n_2 - y}.
\]
Thus, $Y \sim \dBin(n_1 + n_2, p)$.
\end{proof}

\begin{prop}\label{c2p4}
If $X_1 \sim \dPoi(\lambda_1)$ and $X_2 \sim \dPoi(\lambda_2)$ then $X_1 +
X_2 \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{prop}
\begin{proof}
Let $Y = X_1 + X_2$. If $Y = x, X_1 = x_1$ then $X_2 = x - x_1$. Thus,
\begin{eqnarray*}
f_Y(x) = P(Y = x) &=& \sum_{k=0}^xP(X_1 = k, X_2 = y - k) \\
 &=& \sum_{k=0}^xP(X_1=k)P(X_2=y-k) \\
 &=& e^{-\lambda_1}\frac{\lambda_1^{k}}{k!}e^{-\lambda_2}\sum_{k=0}^x
 \frac{\lambda_2^{x - k}}{(x - k)!} \\
 &=& e^{-\lambda_1 - \lambda_2}\sum_{k=0}^x
 \frac{\lambda_1^{k}\lambda_2^{x-k}}{k!(x-k)!} \\
 &=& \frac{e^{-\lambda_1 - \lambda_2}}{x!}\sum_{k=0}^x
 \frac{x!}{k!(x-k)!}\lambda_1^{k}\lambda_2^{x-k} \\
 &=& e^{-(\lambda_1 + \lambda_2)}\frac{(\lambda_1 + \lambda_2)^x}{x!}.
\end{eqnarray*} 
Thus, $Y \sim \dPoi(\lambda_1 + \lambda_2)$.
\end{proof}

\begin{prop}\label{c2p5}
If $X \sim t(\nu)$ then as $\nu \rightarrow \infty$, $X \sim \dNor(0, 1)$.
\end{prop}
\begin{proof}
We will use Striling approximation for $\Gamma$ function,
\begin{equation}\label{c2e24}
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
			\left(1 + O\left(\frac{1}{z}\right)\right)
\end{equation}
For large $z$, it becomes
\[
\Gamma(z) = \sqrt{\frac{2\pi}{z}}\left(\frac{z}{e}\right)^z
\]
The $t$ density is
\[
f(x) = \frac{1}{\sqrt{\pi\nu}}\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)}
	\left(1 + \frac{x^2}{\nu}\right)^{-\nu/2}\left(1 + \frac{x^2}{\nu}\right)^{-1/2}
\]
so that
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{1/2} &=& 1 \\
\lim_{\nu\rightarrow\infty}\left(1 + \frac{x^2}{\nu}\right)^{\nu/2} &=& e^{-x^2/2}
\end{eqnarray*}
and
\begin{eqnarray*}
\lim_{\nu\rightarrow\infty}\frac{1}{\sqrt{\pi\nu}}
\frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\nu/2)} &=& \frac{1}{\sqrt{2e}}
\sqrt{\frac{\nu}{\nu+1}}\left(1 + \frac{1}{\nu}\right)^{\nu/2}\sqrt{\nu+1} \\
 &=& \frac{\sqrt{\nu}}{\sqrt{2e}}\sqrt{e} \\
 &=& \frac{1}{\sqrt{2}}
\end{eqnarray*}
so that
\[
\lim_{\nu\rightarrow\infty}f(x) = \frac{1}{\sqrt{2}}e^{-x^2},
\]
which is the standard normal density.
\end{proof}

A few more propositions about univariate distributions can be proved easily after
introducing moments and generating functions.

\begin{defn}\label{c2d11}
Let $X$ and $Y$ be discrete random variables. The joint pmf of $X$ and $Y$ 
measures the probability of the event $X = x$ and $Y = y$. Thus,
$f(x, y) = P(X = x, Y = y)$.
\end{defn}

\begin{defn}\label{c2d12}
Let $X$ and $Y$ be continuous random variables. The joint density function of
$X$ and $Y$, $f: \sor^2 \rightarrow [0, \infty)$ his a function for which
\[
\iint_{\sor^2} f(x, y)dxdy = 1
\]
and if If $A \subset \sor^2$ then
\[
P((x, y) \in A) = \iint_A f(x, y)dxdy.
\]
\end{defn}

In either case, the cdf is defined as $F_{X, Y}(x, y) = P(X \le x, Y \le y)$.

\begin{defn}\label{c2d13}
If $f(x, y)$ is the joint probability function of discrete random variables then
the marginal pmf of $X$ is
\[
f_X(x) = \sum_y f(x, y)
\]
and the marginal pmf of $Y$ is
\[
f_Y(y) = \sum_x f(x, y).
\]
\end{defn}

\begin{defn}\label{c2d14}
If $f(x, y)$ is the joint probability function of continuous random variables then
the marginal density function of $X$ is
\[
f_X(x) = \int_{-\infty}^\infty f(x, y) dy
\]
and the marginal pmf of $Y$ is
\[
f_Y(y) = \int_{-\infty}^\infty f(x, y) dx.
\]
\end{defn}

\begin{defn}\label{c2d15}
If $X$ and $Y$ are discrete random variables with joint pmf $f(x, y)$ then the
conditional pmf of $X$ given $Y = y$ is
\[
f_{X|Y=y} = \frac{f(x, y)}{f_Y(y)},
\]
provided $f_Y(y) \ne 0$. The conditional pmf of $Y$ given $X = x$ is defined 
similarly.
\end{defn}

\begin{defn}\label{c2d16}
If $X$ and $Y$ are continuous random variables with joint density $f(x, y)$ then
the conditional density of $X$ given $Y = y_0$ is
\[
f_{X|Y = y_0}(x) = \frac{f(x, y_0)}{f_Y(y_0)},
\]
provided $f_Y(y) \ne 0$ and
\[
P(X \in A|Y = y_0) = \int_A f_{X|Y=y_0}dx.
\]
\end{defn}
Note that for a particular value of $y = y_0$, $f_{X|y=y_0}$ is a function of $x$
alone.

\begin{defn}\label{c2d17}
If $X$ and $Y$ are discrete random variables with joint pmf $f(x, y)$ then they
are independent if for all $A, B \in \mathcal{F}$, $P(X \in A, Y \in B) = 
P(X \in A)P(X \in B)$. Likewise, if $X$ and $Y$ are continuous random variables 
with joint density $f(x, y)$ then they are independent if $f(x, y) = f_X(x)f_Y(y)$ 
for all $x, y$.
\end{defn}

\begin{prop}\label{c2p6}
If $X$ and $Y$ are discrete random variables then they are independent iff
$f(x, y) = f_X(x)f_Y(y)$ for all $x, y$.
\end{prop}
\begin{proof}
Let $A = \{x\}$ and $B = \{y\}$, where $x, y$ are arbitrary values of $X$ and
$Y$. Then by definition \ref{c2d17}, $P(X=x, Y=y) = P(X=x)P(Y=y) \Rightarrow
f(x, y) = f_X(x)f_Y(y)$.

Since $X$ and $Y$ are discrete random variables, the sets $A$ and $B$ of their
values are countable. Therefore,
\begin{eqnarray*}
P(X \in A, Y \in B) &=& \sum_{x \in A, y \in B}P(X = x, Y = y) \\
 &=& \sum_{x \in A, y \in B}f(x, y) \\
 &=& \sum_{x\in A, y \in B}f_X(x)f_Y(y) \\
 &=& \sum_{x \in A}f_X(x)\sum_{y \in B}f_Y(y) \\
 &=& P(X \in A)P(Y \in B).
\end{eqnarray*}
\end{proof}

\begin{prop}\label{c2p7}
Let $X$ and $Y$ be continuous random variables and $f$ be their joint density.
If $f(x, y) = g(x)h(y)$ where $g, h: \sor \mapsto \sor$ are functions, not
necessarily densities, then $X$ and $Y$ are independent.
\end{prop}
\begin{proof}
For a fixed $x_0$,
\[
f_X(x_0) = \int_{-\infty}^\infty f(x_0, y)dy = g(x_0)\int_{-\infty}^\infty h(y)dy.
\]
Similarly, for a fixed $y_0$,
\[
f_Y(y_0) = h(y_0)\int_{-\infty}^\infty g(x)dx
\]
so that
\begin{eqnarray*}
f_X(x_0)f_Y(y_0) &=& 
	g(x_0)h(y_0)\int_{-\infty}^\infty h(y)dy\int_{-\infty}^\infty g(x)dx \\
 &=& f(x_0, y_0)\iint_{\sor^2}g(x)h(y)dxdy \\
 &=& f(x_0, y_0)\iint_{\sor^2}f(x, y)dxdy \\
 &=& f(x_0, y_0).
\end{eqnarray*}
The points $x_0, y_0$ were chosen arbitrarily. Therefore, this relation is true
for all $x, y \in \sor$ making $X$ and $Y$ independent.
\end{proof}

\begin{prop}\label{c2p8}
Let $r$ be a strictly monotone function, $X$ be a random variable and $Y=r(X)$.
Then
\[
f_Y(y) = f_X(s(y))\left|\td{s}{y}\right|,
\]
where $s = r^{-1}$.
\end{prop}
\begin{proof}
$F_Y(y) = P(Y \le y) = P(r(X) \le y)$. Since $r$ is monotone, $r^{-1}$ exists so
that $F_Y(y) = P(X \le r^{-1}(y)) = F_X(s(y))$ and hence
\[
f_Y(y) = \td{F_Y}{y} = f_X(s(y))\td{s}{y}.
\]
Since $f_Y$ must be non-negative, we choose the modulus of the derivative of 
$s$.
\end{proof}

The definitions of joint, marginal and conditional distributions for a pair of
random variables can be generalised to a finite number $n$ of random variables.
In particular,
\begin{defn}\label{c2d18}
If $X_1, \ldots, X_n$ are independent, that is the joint pmf or density $f$ 
factorises as
\[
f_{\vec{X}}(x_1, \ldots, x_n) = f(x_1)\cdots f(x_n)
\]
then we say that $X_1, \ldots, X_n$ are independent and identically distributed
, also called iid, and write $X_1, \ldots, X_n \sim f$ or $X_1, \ldots, X_n \sim
F$, if $F$ is the cdf of $f$. We also call $x_1, \ldots, x_n$ to be a random 
sample of size $n$ from $f$ (or $F$).
\end{defn}

\begin{defn}\label{c2d19}
A vector $\vec{X} = (X_1, \ldots, X_n)$, where $X_i$ are non-negative integers,
is said to have a multinomial distribution with parameters $n$ and $\vec{p}$,
denoted as $\vec{X} \sim \dMul(n, \vec{p})$, if $p_1 + \cdots + p_n = 1$ and 
\begin{equation}\label{c2e25}
f(\vec{x}) = \binom{n}{x_1 \ldots x_n}p_1^{x_1}\cdots p_n^{x_n}.
\end{equation}
\end{defn}

\begin{defn}\label{c2d20}
A vector $\vec{X}$ has a multivariate normal distribution, denoted as $\vec{X}
\sim \dNor(\vec{\mu}, \Sigma)$ if it has a density
\begin{equation}\label{c2e26}
f(\vec{x}; \vec{\mu}, \Sigma) = \frac{1}{(2\pi)^{n/2}|\Sigma|^{1/2}}
\exp\left(-\frac{1}{2}(\vec{x} - \vec{\mu})^T\Sigma(\vec{x} - \vec{\mu})\right).
\end{equation}
The positive-definite matrix $\Sigma$ is called the covariance matrix and 
$|\Sigma|$ is its determinant.
\end{defn}

\section{Solutions to problems}
\begin{enumerate}
\item[1.] $F(x) = P(X \le x)$ so that $F(x^\pm) = P(X \le x^\pm)$. Now we have
the set relation $\{X \le x^-\} \cup \{X = x\} = \{x \le x^+\}$ and the union
is of two disjoint sets. Therefore, $P(\{X \le x^-\})+P(X = x)=P(\{X \le x^+\})$
so that $P(X=x) = F(x^+) - F(x^-)$.

\item[2.] We first check that $P(2) + P(3) + P(5) = 1$ so that we have a valid
pmf. The cdf is $F(x) = 0$ for $x < 1$, $F(2) = 0.1, F(3) = 0.2, F(5) = 1$ and 
$F(x) = 1$ for $x > 1$. Figure \ref{c2f1} shows the plot.
\begin{figure}
\includegraphics[scale=0.6]{c2p2}
\caption{Problem 2}\label{c2f1}
\end{figure}

\item[3.] Refer to proposition \ref{c2p2}.

\item[4.] Give that
\[
f(x) = \begin{cases}
0.25 & \text{ if } x \in (0, 1) \\
0.375 & \text{ if } x \in (3, 5) \\
0 & \text{ otherwise.}
\end{cases}
\]
Clearly, 
\[
\int_{-\infty}^\infty f(x)dx = 0.25 \times (1 - 0) + 0.375 \times (5 - 3) = 1
\]
so that $f$ is a valid density. The cdf $F$ is calculated as follows: 
\[
F(x) = \begin{cases}
0 & \text{ if } x \le 0 \\
\frac{x}{4} & \text{ if } x \in (0, 1) \\
\frac{1}{4} & \text{ if } x \in [1, 3] \\
\frac{1}{4} + \frac{3}{8}x & \text{ if } x \in (3, 5) \\
1 & \text{ if } x \ge 5
\end{cases}
\]

Let $Y = 1/X$. $0 < y \le 1/5 \Rightarrow x \ge 5$ so that
\[
F_Y(1/5) = \int_5^\infty f(x) dx = 0.
\]
$1/5 < y < 1/3 \Rightarrow x \in (3, 5)$ so that
\[
F_Y(1/3) = F_Y(1/5) + \int_3^5 f(x)dx = \int_3^5\frac{3}{8}dx = \frac{3}{4}.
\]
$1/3 \le y \le 1 \Rightarrow x \in [1, 3]$ so that
\[
F_Y(1) = F_Y(1/3) + \int_1^3 f(x)dx = \frac{3}{4} + 0 = \frac{3}{4}.
\]
$y > 1 \Rightarrow x \in (1, 1/y)$ so that
\[
F_Y(y) = F_Y(1) + \int_{1/y}^1 f(x)dx 
= \frac{3}{4} + \frac{1}{4}\left(1 - \frac{1}{y}\right).
\]
Thus,
\[
F_Y(y) = \begin{cases}
0 & \text{ if } y <= 1/5 \\
\frac{3}{4} & \text{ if } y \in (1/5, 1/3) \\
\frac{3}{4} & \text{ if } y \in [1/3, 1] \\
\frac{3}{4} + \frac{1}{4}\left(1 - \frac{1}{y}\right) & \text{ if } y > 1
\end{cases}
\]

\item[5.]Refer to proposition \ref{c2p6}.

\item[6.]$Y$ is a Bernoulli random variable. $Y(x) = 1$ iff $x \in A$, $Y(x) = 0$
otherwise. Now,
\[
P(Y = 1) = \int_A f_X(x)dx = f_Y(1)
\]
and
\[
P(Y = 0) = \int_{A^c}f(x) dx = f_Y(0).
\]
Clearly, $P(Y=0) + P(Y=1) = 1$ so that 
\[
F_Y(y) = \begin{cases}
0 & \text{ if } y < 0 \\
\int_A f_X(x)dx & \text{ if } y \in [0, 1) \\
1 & \text{ if } y \ge 1.
\end{cases}
\]

\item[7.] Given that $X \sim \dUni(0, 1)$ and $Y \sim \dUni(0, 1)$ are independent.
Let $Z = \min(X, Y)$. Therefore, $P(Z > z) = P(\min(X, Y) > z) = P(X > z
\cup Y > z) = P(X > z) + P(Y > z) - P(X > z \cap Y > z) = P(X > z) + 
P(Y > z) - P(X > z)P(Y > z)$, as $X$ and $Y$ are independent. Now, $P(X > z) =
1 - P(X \le z) = 1 - F_X(z)$ and similarly for $P(Y > z)$. Therefore, $P(Z > z)
= (1 - F_X(z)) + (1 - F_Y(z)) - (1 - F_X(z))(1 - F_Y(z))$. The cdf for uniform
distribution is $F_X(z) = z$ so that
\[
P(Z > z) = 2(1 - z) - (1 - z)^2
\]
so that $F_Z(z) = P(Z \le z) = 1 - P(Z > z) = 1 - 2(1 - z) + (1 - z)^2 = z^2$
so that $f_Z(z) = 2z$ is the required density.

\item[8.] If $X^+ = \max(X, 0)$ then $X^+$ will never have negative values. 
Therefore, if $G$ is its cdf then $G(y) = 0$ if $y < 0$. For $X \ge 0$, $X^+=X$
so that $P(G \le y)$ for $y > 0$ is $P(X \le y) = F_X(y)$. Thus,
\[
G(y) = \begin{cases}
0 & \text{ if } y < 0 \\
F_X(y) & \text{ if } y \ge 0.
\end{cases}
\]

\item[9.] From equation \eqref{c2e16},
\[
F(x) = 1 - \exp\left(-\frac{x}{\beta}\right).
\]
If $F(x) = q$ then $e^{x/\beta} = 1 - q$ so that $x = \beta\ln(1 - q)$, that is
\begin{equation}\label{c2e27}
F^{-1}(q) = \beta\ln(1 - q).
\end{equation}

\item[10.] Let $U = g(X), V = h(Y)$ be random variables which are functions of 
$X$ and $Y$. Therefore, $g^{-1}(a)$ is a subset of $\sor$ whose values are image
of a subset of the sample space under $X$. Similarly for $h^{-1}(b)$. Since $X$
and $Y$ are independent, $P(X \in g^{-1}(a), Y \in h^{-1}(b)) = P(X \in g^{-1}(a))
P(Y \in h^{-1}(b)) \Rightarrow P(g(X) = a, h(Y) = b) = P(g(X)=a)P(h(Y)=b) 
\Rightarrow P(U=a, V=b) = P(U=a)P(V=b)$. This proof is applicable to discrete, 
$X, Y$.

If $X$ and $Y$ are continuous, 
\[
f(x=g^{-1}(a), y=g^{-1}(b)) = f_X(x=g^{-1}(a))f_Y(y=h^{-1}(b)) 
\]
so that $f(g(x)=a, h(y)=b) = f_X(g(x)=a)f_Y(h(y)=b) 
\Rightarrow f^\ast(u=a, v=b) = f_X^\ast(u=a)f_Y^\ast(v=b)$, where $f_X^\ast(u)
= f_X(g(x))$ and analogously for other functions.

\item[11.] Suppose we toss the coin $n$ times. If $h, t \le n$ are the number of 
heads and tails, then $P(h = x, t = y)$ will be zero whenever $x + y \ne n$, yet
$P(h = x)$ and $P(t = y)$ are not zero. Therefore, the two random variables are 
not independent.

Let $H$ be the random variable measuring the number of heads, $T$ measuring tails
and $N$ the number of tosses. Then $P(H = h, T = t, N = n) = P(H=h, T=t|N=n)
P(N=n) = P(H=h, T=n-h)P(N=n)$. Now,
\[
P(H=h, T=n-h) = \binom{n}{h}p^{h}(1 - p)^{n-h}
\]
and
\[
P(N=n) = e^{-\lambda}\frac{\lambda^n}{n!}
\]
so that
\begin{eqnarray*}
P(H = h, T = t, N = n) &=& 
	\frac{n!}{h!(n-h)!}p^h(1-p)^{n-h}e^{-\lambda}\frac{\lambda^n}{n!} \\
 &=& \frac{p^h}{h!}\frac{(1-p)^{n-h}}{(n-h)!} e^{-\lambda}\lambda^h \lambda^{n-h} \\
 &=& \frac{(p\lambda)^h}{h!}\frac{((1-p)\lambda)^{n-h}}{(n-h)!}e^{-(p + (1 - p))\lambda} \\
 &=& e^{-p\lambda}\frac{(p\lambda)^h}{h!} e^{-q\lambda}\frac{(q\lambda)^{n-h}}{(n-h)!},
\end{eqnarray*}
where we used $q = 1 - p$ for notational simplicity. The right hand side is a 
product of two Poisson random variables with parameters $p\lambda$ and $q\lambda$
and values $h$ and $n - h$. Thus, $P(H=h, T=t, N=n)$ factorises as a product of
two random variables whose sum is always $n$.

\item[12.] Refer to proposition \ref{c2p7}.

\item[13.] Given that $X \sim \dNor(0, 1)$ so that its cdf is
\[
\Phi(x) = \frac{1}{2}\left[1 + \erf\left(\frac{x}{\sqrt{2}}\right)\right].
\]
If $Y = \exp(X)$ then $F_Y(y) = P(Y \le y) = P(\exp(X) \le y) = P(X \le \ln Y)$
so that
\begin{equation}\label{c2e28}
F_Y(y) = \frac{1}{2}\left[1 + \erf\left(\frac{\ln y}{\sqrt{2}}\right)\right].
\end{equation}
The variable $Y$ is a log-normal distribution. Its density is
\[
f_Y(y) = \td{F_Y}{y} = 
\frac{1}{2}\frac{2}{\sqrt{\pi}}\exp\left(-\frac{(\ln y)^2}{2}\right)
\frac{1}{y\sqrt{2}},
\]
where we used the fact that
\[
\frac{d}{dx}\erf(x) = \frac{2}{\sqrt{\pi}}\exp(-x^2).
\]
Upon simplification,
\begin{equation}\label{c2e29}
f_Y(y) = \frac{1}{y\sqrt{2\pi}}\exp\left(-\frac{(\ln y)^2}{2}\right)
\end{equation}

A comparison of the histogram of randomly generated log-normal data with its
theoretical density given by \eqref{c2e29} is shown in figure \ref{c2f2}
\begin{figure}
\centering
\includegraphics[scale=0.6]{c2p13}
\caption{Problem 13}\label{c2f2}
\end{figure}

\item[14.] Given that $X$ and $Y$ are uniformly distributed over the unit disc.
If $R = \sqrt{X^2 + Y^2}$ then $F_R(r) = P(R \le r) = (\pi r^2)/\pi = r^2$ so 
that $f_R = F_R^\op = 2r$ is the density function.

\item[15.] Given that $X$ has a continuous, strictly increasing cdf $F$. That is,
$F^{-1}$ exists. Let $Y = F(X)$. Then $F_Y(y) = P(Y \le y) = P(F(X) \le y)$. 
Since $F^{-1}$ exits, $F_Y(y) = P(X \le F^{-1}(y)) = F_X(F^{-1}(y))$.

If $U \sim \dUni(0, 1)$ then $F_U(u) = u$. If $X = F^{-1}(U)$ for some function
$F$ then $F_X(x) = P(X \le x) = P(F^{-1}(U) \le x) = P(U \le F(x)) = F_U(F(x)) =
F(x)$. Therefore, $X \sim F$. Thus, a uniform distribution can be transformed into 
any other distribution.

From \eqref{c2e16}, the cdf of an exponential random variable is
\[
F(x) = 1 - \exp\left(-\frac{x}{\beta}\right) = y,
\]
say. Then 
\[
\exp\left(-\frac{x}{\beta}\right) = 1 - y \Rightarrow -\frac{x}{\beta} = 
\log(1 - y) \Rightarrow x = -\beta\log(1 - y)
\]
that is $F^{-1}(y) = -\beta\log(1 - y)$. The Python code confirms that the random
variable generated using this algorithm is needed of the correct type.
\begin{verbatim}
import numpy as np
import scipy.stats as scstat

rng = np.random.default_rng()
size = 1000
U = rng.uniform(low=0, high=1, size=size)

# Scale parameter of the distribution.
beta = 2
# Exponential rv generated using the algorithm.
X = [-beta * np.log(1 - u) for u in U]
# Exponential rv generated by the RNG.
E = rng.exponential(scale=beta, size=size)

result = scstat.ks_2samp(X, E, alternative='two-sided')
H0 = 'X has exponential distribution with parameter {0}.'.format(beta)
if result.pvalue < 0.05:
    print('Reject H0: {0}'.format(H0))
else:
    print('Fail to reject H0: {0}'.format(H0))
\end{verbatim}
The hypothesis test fails to reject the null hypothesis.

\item[16.] Let $X \sim \dPoi(\lambda), Y \sim \dPoi(\mu)$ and let $Z = X + Y$. 
The conditional density of $X$ given $Z = n$ is,
\[
f_{X|Z=n}(x) = \frac{f(x, n)}{f_Z(n)}.
\]
The event $X = x, Z = n$ is the same as $X = x, Y = n - x$ and its probability,
given the independence of $X$ and $Y$, is $P(X=x)P(Y=n-x)$. Using \eqref{c2e9},
it is,
\[
f(x, n) = e^{-\lambda}\frac{\lambda^x}{x!} e^{-\mu}\frac{\mu^{n-x}}{(n-x)!}.
\]
The probability of the event $Z = n$ is
\begin{eqnarray*}
f_Z(n) &=& \sum_{x=0}^nP(X=x)P(Y=n-x) \\
 &=& \sum_{x=0}^n e^{-\lambda}\frac{\lambda^x}{x!} e^{-\mu}\frac{\mu^{n-x}}{(n-x)!} \\
 &=& \frac{e^{-(\lambda+\mu)}}{n!}\sum_{x=0}^n\binom{n}{x}\lambda^x\mu^{n-x} \\
 &=& \frac{e^{-(\lambda+\mu)}}{n!}(\lambda + \mu)^n
\end{eqnarray*}
so that
\begin{eqnarray*}
f_{X|Z=n}(x) &=& \frac{e^{-(\lambda+\mu)}\lambda^x\mu^{n-x}}{x!(n-x)!}
     \frac{n!}{e^{-(\lambda+\mu)}}\frac{1}{(\lambda + \mu)^n} \\
 &=& \binom{n}{x}\left(\frac{\lambda}{\lambda+\mu}\right)^x
     \left(\frac{\mu}{\lambda+\mu}\right)^{n-x}
\end{eqnarray*}
If 
\[
\pi = \frac{\lambda}{\lambda+\mu}
\]
then 
\[
f_{X|Z=n}(x) = \binom{n}{x}\pi^x(1 - \pi)^{n-x},
\]
which is the pmf of $\dBin(n, \pi)$.

\item[17.] Given that
\[
f(x, y) = \begin{cases}
c(x^2 + y^2) & \text{ if } x \in [0, 1], y \in [0, 1] \\
0 & \text{ otherwise.}
\end{cases}
\]
For $f$ to be a valid density, we require that
\[
\iint_{\sor^2}f(x, y)dxdy = 1.
\]
That is,
\[
c\int_0^1\int_0^2 (x^2 + y^2)dxdy = c\left(\int_0^1x^2dx\int_0^1 dy + 
\int_0^1dx\int_0^1 y^2dy\right) = \frac{2c}{3}.
\]
We this require $c = 3/2$. Thus, we have 
\[
f(x, y) = \frac{3}{2}(x^2 + y^2)
\]
for $x \in [0, 1], y \in [0, 1]$ and zero everywhere else. Therefore,
\[
P(X < 1/2|y = 1/2) = \int_0^{1/2}\frac{3}{2}\left((x^2 + \frac{1}{4}\right)dx = 
\frac{1}{4}.
\]

\item[18.] Solved in the Python notebook.

\item[19.] Refer to proposition \ref{c2p8}.

\item[20.] Given that $X, Y \sim \dUni(0, 1)$. Let $Z = X - Y$. The range of $Z$
is $(-1, 1)$. Therefore, we expect $f_Z$ to be non-zero on $(-1, 1)$. Let 
$z < 0$. For a given $x$, $y$ ranges from $-z$ to $1$. Therefore,
\[
f_Z(z) = \int_0^1f_X(x)dx\int_{-z}^1f_Y(y)dy = (1 + z).
\]
If $z > 0$, then for a given $x$, $y$ ranges from $0$ to $1 - z$ so that
\[
f_Z(z) = \int_0^1f_X(x)dx\int_0^{1 - z}f_Y(y)dy = 1 - z.
\]
Thus,
\begin{equation}
f_Z(z) = \begin{cases}
1 + z & \forall z \in (1, 0) \\
1 - z & \forall z \in (0, 1)
\end{cases}
\end{equation}

Now let $Z = X/Y$. We will first find the cdf. 
\begin{eqnarray*}
F_Z(z) &=& P(Z \le z) = P(X \le Yz) \\
 &=& \int_0^1\int_0^{\min(1, yz)}f_X(x)f_Y(y)dxdy \\
 &=& \int_0^1\int_0^{\min(1, yz)}dxdy \\
 &=& \int_0^{\min(1, yz)}dy \\
 &=& \int_0^1 \min(1, yz)dy,
\end{eqnarray*}
where the last step follows from the fact that $\int_0^a dy = \int_0^1 ady$.
Refer to figure \ref{c2f3} to understand the range of integration.
\begin{figure}
\centering
\includegraphics[scale=0.5]{c2p20}
\caption{Range of integration}\label{c2f3}
\end{figure}
If $z < 1$ then $y$ can range over $(0, 1)$ and yet keep $yz$ less than $1$.
The probability of this happening is the area of the triangle OCB. It is
\[
\int_0^1 \min(1, yz)dy = \int_0^1 yz dy = \frac{z}{2}.
\]
If $z >= 1$ then $\min(1, yz) = yz$ for $y \in (0, 1/z)$ and $\min(1, yz) = 1$
for $y \in (1/z, 1)$. Ths probability of this event is the area of the 
quadrilateral OADB. It can be calculated as the area of the triangle OAE and
the area of the rectangle ADCE. It is
\[
\int_0^{1/z}yzdy + \int_{1/z}^1 dy = \frac{1}{2z} + 1 - \frac{1}{z} = 
1 - \frac{1}{2z}
\]
so that
\[
F_Z(z) = \begin{cases}
\frac{z}{2} & \forall z \in (0, 1) \\
1 - \frac{1}{2z} & \forall z >= 1
\end{cases}
\]
so that the pdf is
\[
f_Z(z) = \begin{cases}
\frac{1}{2} & \forall z \in (0, 1) \\
\frac{1}{2z^2} & \forall z \ge 1.
\end{cases}
\]

\item[21.] Let $X_1, \ldots, X_n \sim \dExp(\beta)$ be iid and let $Y = 
\max\{X_1, \ldots, X_n\}$. Then, $F_Y(y) = P(Y \le y)$. This is true iff $y$ is
greater than all $X_1, \ldots, X_n$. Thus,
\[
F_Y(y) = \prod_{i=1}^n P(X_i \le y) = \prod{i=1}^n(1 - e^{-y/\beta}) = 
(1 - e^{-y/\beta})^n
\]
so that
\[
f_Y(y) = \frac{n}{\beta}(1 - e^{-y/\beta})^{n-1}e^{-y/\beta}.
\]
\end{enumerate}
\end{document}
