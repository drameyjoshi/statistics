{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6683ad0c-5921-4993-939a-96dd2e718745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6c6cc-3e8a-413b-9514-51a137133313",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = 'galton.csv'\n",
    "pathname: str = os.path.join('.', 'datasets', dataset)\n",
    "\n",
    "if os.path.isfile(pathname):\n",
    "    df: pd.core.frame.DataFrame = pd.read_csv(pathname)\n",
    "else:\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fd0cc-801c-45bf-8994-18b3794fb2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d74167-53ba-4d0f-8a36-c0dbe6efc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "df.hist('child', ax=ax[0])\n",
    "_ = df.hist('parent', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c468be-712e-4f8a-a149-961e9f80d50e",
   "metadata": {},
   "source": [
    "# Minimal square error\n",
    "\n",
    "Given a data set $x_1, \\ldots, x_n$, find a $\\mu$ such that \n",
    "$$\n",
    "\\sum_{i=1}^n (x_i - \\mu)^2\n",
    "$$\n",
    "is minimal.\n",
    "\n",
    "This is a simple problem in calculus. Alternatively, we can also confirm numerically that the minimum is\n",
    "$$\n",
    "\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6adff-625c-4bfc-b8c1-06f14c159170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqerr(mu: float, X: np.array) -> float:\n",
    "    return np.sum((X - mu)**2)\n",
    "\n",
    "res: scipy.optimize._optimize.OptimizeResult = None\n",
    "res = scipy.optimize.minimize(fun=sqerr, \n",
    "                              x0=df['child'].iloc[0], \n",
    "                              args=df['child'].to_numpy(), \n",
    "                              method='nelder-mead')\n",
    "if res.success:\n",
    "    minimum_at = res.x\n",
    "    print(f'Minimum found by optimiser: {np.round(res.x[0], 6)}')\n",
    "    print(f'Expected minimum: {np.round(df['child'].mean(), 6)}')\n",
    "else:\n",
    "    print('The optimiser did not converge.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce22147-fd81-4c62-92dd-712a14e92b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.plot(x='parent', y='child', kind='scatter', title=\"Galton's data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a523-6b5c-4115-a728-f39c79276c72",
   "metadata": {},
   "source": [
    "# Subtract mean from both variables\n",
    "Let $\\tilde{x}_i = x_i - \\bar{x}, \\tilde{y}_i = y_i - \\bar{y}$. This is no more than a translation of the origin. We will now find the best slope $\\beta$ that minimises\n",
    "$$\n",
    "\\sum_{i=1}^n (\\tilde{y}_i - \\beta \\tilde{y}_i)^2.\n",
    "$$\n",
    "It turns out that it is the slope of the regression line. This also suggests that the point $(\\bar{x}, \\bar{y})$ will lie on the regression line.\n",
    "\n",
    "The variables $\\tilde{x}_i$ and $\\tilde{y}_i$ are called the _centred_ versions of $x_i$ and $y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0878ed-7644-4b84-8cb4-9ab4142545ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prime = df['parent'].to_numpy() - df['parent'].mean()\n",
    "y_prime = df['child'].to_numpy() - df['child'].mean()\n",
    "\n",
    "def slope(beta: float, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "    return np.sum((Y - beta*X)**2)\n",
    "\n",
    "res = scipy.optimize.minimize(fun=slope, \n",
    "                              x0=1, \n",
    "                              args=(x_prime, y_prime), \n",
    "                              method='nelder-mead')\n",
    "if res.success:\n",
    "    minimum_at = res.x\n",
    "    print(f'Minimum found by optimiser: {res.x[0]}')\n",
    "else:\n",
    "    print('The optimiser did not converge.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614cabda-a7a0-44c7-b80b-feff2509a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(endog=y_prime, exog=x_prime)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d642b9ea-9e34-4c62-b94a-b5c9c69df9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Minimum found by optimiser: {np.round(res.x[0], 6)}')\n",
    "print(f'Expected minimum: {np.round(result.params[0], 6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276f124-f03e-4126-bb47-3fdd9abc4b05",
   "metadata": {},
   "source": [
    "# Basic stats about the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbaed0-b671-4fea-81f3-353c3a232059",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bar = df['parent'].mean()\n",
    "x_var = df['parent'].var()\n",
    "s_x = df['parent'].std()\n",
    "\n",
    "y_bar = df['child'].mean()\n",
    "y_var = df['child'].var()\n",
    "s_y = df['child'].std()\n",
    "\n",
    "cov_xy = np.cov(df['parent'].to_numpy(), df['child'].to_numpy())\n",
    "cor_xy = np.corrcoef(df['parent'].to_numpy(), df['child'].to_numpy())\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c11068-2ac4-4fea-98e3-63186d9341a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Empirical mean of x',\n",
    "           'Empirical variance of x',\n",
    "           'Empirical standard deviation of x',\n",
    "           'Empirical mean of y',\n",
    "           'Empirical variance of y',\n",
    "           'Empirical standard deviation of y',\n",
    "           'Empirical covariance',\n",
    "           'Empirical correlation']\n",
    "values = [x_bar, x_var, s_x, y_bar, y_var, s_y, cov_xy[0][1], cor_xy[0][1]]\n",
    "basic_stats = pd.DataFrame({'metric': metrics, 'value': values})\n",
    "basic_stats.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce422395-1245-4c7f-8a67-9cbe3e4f3544",
   "metadata": {},
   "source": [
    "# Theoretical estimates of regression parameters\n",
    "\n",
    "It can be shown that if $y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i$ then\n",
    "$$\n",
    "\\hat{\\beta}_1 = \\frac{s_y}{s_x} \\rho_{xy}\n",
    "$$\n",
    "and $\\hat{\\beta}_0 = \\bar{y} - \\beta_1\\bar{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9601d-4d0f-45dc-bd57-1b47f65c6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1_hat_th = cor_xy * s_y/s_x\n",
    "beta_0_hat_th = y_bar - beta_1_hat_th * x_bar\n",
    "\n",
    "model = sm.OLS(endog = df['child'], exog=sm.add_constant(df['parent']))\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f2495-4bce-4b49-984c-5731efe5fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_metrics = ['Theoretical intercept',\n",
    "               'Theoretical slope',\n",
    "               'Fitted intercept',\n",
    "               'Fitted slope']\n",
    "reg_values = [beta_0_hat_th, beta_1_hat_th, result.params.iloc[0], result.params.iloc[1]]\n",
    "reg_stats = pd.DataFrame({'Metric': reg_metrics, 'Value': reg_values})\n",
    "reg_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0fd9a3-b9c0-4131-a077-b63e32ca3d0d",
   "metadata": {},
   "source": [
    "No wonder, they match exactly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bffceb-4099-405f-bb54-8f0a9dea5b94",
   "metadata": {},
   "source": [
    "# Scaled variables \n",
    "If we standardise the variables $x$ and $y$ then the slope of the slope of the regression line will be their correlation coefficient and their intercept will be zero. We will check that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411e908-15e4-43eb-9321-2d1f53377348",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = (df['parent'] - x_bar)/s_x\n",
    "y_std = (df['child'] - y_bar)/s_y\n",
    "model = sm.OLS(endog=y_std, exog=x_std)\n",
    "result = model.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c992f-bb40-4c12-abcb-ceafc144016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Correlation coeff is {np.round(cor_xy[0][1], 4)}, slope of regression line of standardised variables is {np.round(result.params.iloc[0], 4)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
